# Chapter 2
## Exercise
1. 입실론 탐욕적 행동 선택에서 두 개의 행동이 있고 <img src="/rl-introduction-2nd-edit/tex/4f5b0033a3e7c19b1b1e947f7f15ba5a.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>라면 탐욕적 행동을 선택할 확률은 얼마인가?
> 행동이 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>이 있고, 최적 행동이 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>이라고 하자. <img src="/rl-introduction-2nd-edit/tex/4f5b0033a3e7c19b1b1e947f7f15ba5a.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>이기 때문에 <img src="/rl-introduction-2nd-edit/tex/3f19f7d8b2166a43a561f2a4f59dd203.svg?invert_in_darkmode&sanitize=true" align=middle width=78.89817704999999pt height=21.18721440000001pt/>이고 따라서 50% 확률로 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>을 선택하게 된다. 나머지 50% 선택은 가능한 모든 행동을 균등한 확률로(random) 두고 하게 된다. 따라서 <img src="/rl-introduction-2nd-edit/tex/ba7e45b24a513e2e1e0a174b64764d1c.svg?invert_in_darkmode&sanitize=true" align=middle width=113.24196014999998pt height=21.18721440000001pt/>, 즉 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>를 각각 25% 확률로 선택하게 된다. 정리해보면 최적 행동인 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>은 <img src="/rl-introduction-2nd-edit/tex/2e3765e074a4e2165a4f8c6675f1867f.svg?invert_in_darkmode&sanitize=true" align=middle width=121.46116949999997pt height=21.18721440000001pt/>, 75% 확률로 선택되며 <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>는 <img src="/rl-introduction-2nd-edit/tex/512128b1ead18bed472a510dafa27a24.svg?invert_in_darkmode&sanitize=true" align=middle width=29.22385289999999pt height=21.18721440000001pt/>, 25% 확률로 선택된다.

2. **(다중 선택 예제)** 네 개의 행동 중 하나를 선택하는 다중 선택 문제를 생각해 보자. 각 행동은 번호 1, 2, 3, 4로 구분한다.(<img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/f87dcdc9f19dd4b14e54687dc1069783.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>) 이 문제에 입실론 탐욕적 행동 선택을 이용한 다중 선택 알고리즘을 적용한다고 생각해 보자. 이때 이 알고리즘은 표본 평균 행동 가치 추정값을 이용하고 초기 추정값 <img src="/rl-introduction-2nd-edit/tex/3fc01f9433855fb3c7801b5d10862d64.svg?invert_in_darkmode&sanitize=true" align=middle width=71.98131269999999pt height=24.65753399999998pt/>을 적용한다. 시간 단계에 따른 행동 및 가치의 몇몇 초기값들이 <img src="/rl-introduction-2nd-edit/tex/3b3397354be8fbf449fb08f23581c2b4.svg?invert_in_darkmode&sanitize=true" align=middle width=49.84009799999998pt height=22.465723500000017pt/>(=<img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>), <img src="/rl-introduction-2nd-edit/tex/73b17b116c0246f8c9cd793e88b6e768.svg?invert_in_darkmode&sanitize=true" align=middle width=62.778208349999986pt height=22.465723500000017pt/>, <img src="/rl-introduction-2nd-edit/tex/5ad1ac1cd0783ebc1e279ad018f3868e.svg?invert_in_darkmode&sanitize=true" align=middle width=49.84009799999998pt height=22.465723500000017pt/>(=<img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>), <img src="/rl-introduction-2nd-edit/tex/5d4bc9f93aedb17a4a9165561b2e6a9f.svg?invert_in_darkmode&sanitize=true" align=middle width=49.99277579999999pt height=22.465723500000017pt/>, <img src="/rl-introduction-2nd-edit/tex/0f66a171006972f8b33529ff73a3b629.svg?invert_in_darkmode&sanitize=true" align=middle width=49.84009799999998pt height=22.465723500000017pt/>(=<img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>), <img src="/rl-introduction-2nd-edit/tex/0c47f993dc5fcb21d24ca4150f65d28b.svg?invert_in_darkmode&sanitize=true" align=middle width=62.778208349999986pt height=22.465723500000017pt/>, <img src="/rl-introduction-2nd-edit/tex/6d1d41d61a6406497e9ca507622fb5d2.svg?invert_in_darkmode&sanitize=true" align=middle width=49.84009799999998pt height=22.465723500000017pt/>(=<img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>), <img src="/rl-introduction-2nd-edit/tex/2f299fb2f46bd1ac4ddfacb7d0befaa8.svg?invert_in_darkmode&sanitize=true" align=middle width=49.99277579999999pt height=22.465723500000017pt/>, <img src="/rl-introduction-2nd-edit/tex/65afd028d0ba17defc76b0042132ca60.svg?invert_in_darkmode&sanitize=true" align=middle width=49.84009799999998pt height=22.465723500000017pt/>(=<img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>), <img src="/rl-introduction-2nd-edit/tex/a86005552c40f5c1fdaf61689711c758.svg?invert_in_darkmode&sanitize=true" align=middle width=49.99277579999999pt height=22.465723500000017pt/>이라고 가정해보자. 이 시간 단계 중 일부에서 행동이 무작위로 선택되는 입실론 상황이 발행했을 수 있다. 어떤 시간 단계에서 이 상황이 확실하게 발생했을까? 어떤 시간 단계에서 이 상황이 발생하는 것이 가능했을까?

> 시간 순서대로 선택한 것을 기반으로 행동 가치 Q 테이블을 작성하면 아래와 같다. time step마다 선택된 행동은 :heavy_check_mark:로 표시했다.

|action|<img src="/rl-introduction-2nd-edit/tex/78e95e0d458b1115649366ef4e2feb10.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/cdf43ad0366b98bad8d28d8dc150f925.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/26f5cfaae3aa8ccd87fb3958011e4789.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/172c3f2c8c76d04bd95b98119aa143f3.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/065039dafe7712b36d84dc654fa11a5a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|
|:-----|:-----:|:-----:|:-----:|:-----:|:-----:|
|<img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|-1:heavy_check_mark:|-1     |-1     |-1     |-1     |
|<img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|0     |1:heavy_check_mark:|-0.5:heavy_check_mark:|0.333:heavy_check_mark:|0.333  |
|<img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|0     |0      |0      |0      |0:heavy_check_mark:|
|<img src="/rl-introduction-2nd-edit/tex/f87dcdc9f19dd4b14e54687dc1069783.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|0     |0      |0      |0      |0      |

> <img src="/rl-introduction-2nd-edit/tex/bd2646dab26b9eeafdfe02e28f25b633.svg?invert_in_darkmode&sanitize=true" align=middle width=159.27511049999998pt height=40.41699090000003pt/> 수식을 이용하여 계산했다. <img src="/rl-introduction-2nd-edit/tex/26f5cfaae3aa8ccd87fb3958011e4789.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>, <img src="/rl-introduction-2nd-edit/tex/172c3f2c8c76d04bd95b98119aa143f3.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>인 2번의 시간 단계에서 무작위로 선택되는 `입실론 상황`이 발생했다. 먼저 <img src="/rl-introduction-2nd-edit/tex/26f5cfaae3aa8ccd87fb3958011e4789.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>일 때까지 완성된 테이블에서 최적 행동은 <img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>이나 <img src="/rl-introduction-2nd-edit/tex/f87dcdc9f19dd4b14e54687dc1069783.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>였지만, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>가 선택되었기 때문이다. <img src="/rl-introduction-2nd-edit/tex/172c3f2c8c76d04bd95b98119aa143f3.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>일 때 까지의 최적 행동은 <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>였으며 선택은 <img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>을 했으므로 `입실론 상황`이 또 발생한 것으로 볼 수 있다. 따라서 총 2번의 `입실론 상황`이 일어났다.

3. 그림 2.2의 비교 그래프에서 `누적 보상`과 `최고의 행동을 선택할 확률`을 고려할 때 어떤 방법이 장기적으로 가장 좋은 성능을 보여줄 것인가? 얼마나 더 좋을 것인가? 이 문제에 대해 정량적으로 답변해 보라.
> 그림 2.2에서 <img src="/rl-introduction-2nd-edit/tex/b1a0aae36cb870f0a253a00be6779cea.svg?invert_in_darkmode&sanitize=true" align=middle width=58.80698669999999pt height=21.18721440000001pt/>인 상황이 장기적으로 좋은 성능을 보여줄 것으로 예상된다. 

> 그래프 2개에서 각각 시간 단계가 1000까지 나타나 있다. 그래프의 기울기가 완만하면 더 이상 값이 변화하지 않을 것으로 판단되기 때문에 그래프의 추세선 기울기로 앞으로 평균 보상이 더 올라갈 수 있는가를 예상해볼 수 있다. 또 다른 지표로는 최적 행동의 비율로 볼 수 있다. 최적 행동 비율이 높다는 것은 곧 탐험을 통해 최적 행동을 잘 식별하게 되었다는 것을 의미한다. <img src="/rl-introduction-2nd-edit/tex/b4a6eee1a419addbfb7c57f02d3502e5.svg?invert_in_darkmode&sanitize=true" align=middle width=37.80234314999999pt height=21.18721440000001pt/>인 상황은 약 100 step 이전에 평균보상 1과 최적 행동 비율 35%정도에서 정체 되었고 장기적으로 변동을 보이지 않을 만한 평평한 추세선이 그려진다. 즉, 준최적 행동을 수행하는 상황에 걸렸다고 볼 수 있다. 따라서 <img src="/rl-introduction-2nd-edit/tex/946d41bc6d842a13d5592e2a97cc48fd.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>인 상황(A)과 <img src="/rl-introduction-2nd-edit/tex/b1a0aae36cb870f0a253a00be6779cea.svg?invert_in_darkmode&sanitize=true" align=middle width=58.80698669999999pt height=21.18721440000001pt/>인 상황(B)을 비교해보겠다. 시간 단계 1000에서 A는 평균 보상 값이 B보다 0.1정도 높지만 최적 행동의 비율을 보면 완만한 기울기로 정체되어 있으며 더 이상 최적 행동 비율을 높일 가능성 적다. 반면 B는 최적 행동 비율 추세선이 좀 더 가파르며 A와의 평균보상 차이가 크지 않기 때문에 1000단계 이상의 장기적인 관점에서 A보다 B의 평균 보상이 더 높아져서 좋은 성능을 보일 수 있다.

4. 시간 간격의 크기 <img src="/rl-introduction-2nd-edit/tex/69533fe94ebdbe9cbed32f37b8365bb6.svg?invert_in_darkmode&sanitize=true" align=middle width=18.64167029999999pt height=14.15524440000002pt/>이 고정된 값이 아니라면 추정값 <img src="/rl-introduction-2nd-edit/tex/64d0afab00e7391a072599284b91840f.svg?invert_in_darkmode&sanitize=true" align=middle width=21.121448699999988pt height=22.465723500000017pt/>
은 이전까지 받은 보상들의 가중 평균이고, 이때 가중치는 식 2.6에서 주어지는 것과는 다르다. 식 2.6과 유사한 일반적인 경우에 있어서 바로 이전 보상에 적용할 가중치는 얼마인가? 시간 간격의 크기와 관련하여 답변해 보라.
* (식2.6): <p align="center"><img src="/rl-introduction-2nd-edit/tex/c11aebc10ad69cb5193d36f666a1fc61.svg?invert_in_darkmode&sanitize=true" align=middle width=353.8626531pt height=200.76626789999997pt/></p>

> what..?

5. *(programming)* 표본평균 방법을 비정상적(nonstationary) 문제에 적용하기 어렵다는 점을 보여주는 실험을 설계하고 수행하라. 모든 <img src="/rl-introduction-2nd-edit/tex/efc6578f07cd54c96cc674beb6d67c8a.svg?invert_in_darkmode&sanitize=true" align=middle width=36.36998804999999pt height=24.65753399999998pt/>가 동일한 초깃값으로부터 시작하며 독립적으로 무작위 값을 갖도록 변형한 10중 선택 문제를 활용하라(말하자면, 평균이 0이고 표준 편차가 0.01인 정규 분포를 따르는 확률 변수를 각 시간 단계에서 <img src="/rl-introduction-2nd-edit/tex/efc6578f07cd54c96cc674beb6d67c8a.svg?invert_in_darkmode&sanitize=true" align=middle width=36.36998804999999pt height=24.65753399999998pt/>에 더하도록 함으로써). 점증적으로 계산한 표본평균을 활용하는 행동 가치 방법과 고정된 시간 간격(<img src="/rl-introduction-2nd-edit/tex/6bc8e9f14104cfc2a848bdf86f2fca37.svg?invert_in_darkmode&sanitize=true" align=middle width=53.49877169999999pt height=21.18721440000001pt/>)을 사용하는 또 다른 행동 가치 방법에 대해 각각 그림 2.2와 같은 그래프를 그려 보라. <img src="/rl-introduction-2nd-edit/tex/946d41bc6d842a13d5592e2a97cc48fd.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>을 적용하고 더 많은 단계, 말하자면 10,000번의 단계를 적용해 보라.

> [python code](./chapter02/2_5.py)의 [결과](./chapter02/2_5.png)

6. **신비한 스파이크** 그림 2.3에 보이는 결과는 무작위로 선택한 2000번의 10중 선택 결과에 대한 평균이기 때문에 매우 믿을 만하다. 그렇다면 왜 긍정적 방법의 경우 곡선의 처음 부분에서 요동(oscillation)과 스파이크(spike)가 나타나는가? 다시 말하면 무엇이 이 방법의 성능을 평균적으로, 특히 처음 부분에서 특별히 더 좋거나 더 나쁘게 만드는가?
>

7. **편차 없는 고정 시간 간격 기법** 이 장의 대부분에서 행동 가치를 추정하기 위해 표본평균을 이용했다. 그 이유는 표본평균을 사용하면 고정 시간 간격의 경우 발생하는 초기 편하를 없앨 수 있기 때문이다(식 2.6의 분석을 참고하라). 하지만 표본평균은 완전히 만족스러운 해결책은 아니다. 비정상적 문제에서는 형편없는 성능을 보일 수 있기 때문이다. 비정상적 문제에 대해 고정 시간 간격의 장점을 유지하면서도 편차를 없애는 것이 가능할까? 한 가지 방법은 특별한 행동에 대해 n번째 보상을 처리하기 위해 다음과 같은 시간 간격을 이용하는 것이다. <p align="center"><img src="/rl-introduction-2nd-edit/tex/47b54ad0d535806996774291b580371c.svg?invert_in_darkmode&sanitize=true" align=middle width=75.05330745pt height=16.438356pt/></p> 여기서 <img src="/rl-introduction-2nd-edit/tex/0c837ccdd0ecd0a1381239002bf821a4.svg?invert_in_darkmode&sanitize=true" align=middle width=40.713337499999994pt height=21.18721440000001pt/>는 계속 하용하던 고정 시간 간격이고, <img src="/rl-introduction-2nd-edit/tex/ca8330851140ee56eaa532c73abce1c7.svg?invert_in_darkmode&sanitize=true" align=middle width=16.094073599999987pt height=18.666631500000015pt/>는 0에서 시작하는 값의 일반항이다. <p align="center"><img src="/rl-introduction-2nd-edit/tex/818e367c5784b63d62ed81d5c2de49d4.svg?invert_in_darkmode&sanitize=true" align=middle width=300.57123855pt height=16.438356pt/></p> 식 2.6과 같은 분석을 수행하여 <img src="/rl-introduction-2nd-edit/tex/64d0afab00e7391a072599284b91840f.svg?invert_in_darkmode&sanitize=true" align=middle width=21.121448699999988pt height=22.465723500000017pt/>이 초기 편차가 없는 기하급수적 최신 가중 평균임을 보여라.

>
    
8. **UCB 스파이크** 그림 2.4에서 UCB 알고리즘은 11번째 단계의 성능에서 뚜렷한 스파이크를 보여준다. 왜 이런 현상이 생길까? 완전히 만족스러운 답변을 하려면 11번쨰 단계에서 왜 보상이 증가하는지, 그리고 왜 이어지는 단계에서는 감소하는지를 설명해야 한다. 힌트: c=1이면 스파이크가 덜 두드러진다.
>

9. 

## Q&A