# Chapter 2
## Exercise
1. 입실론 탐욕적 행동 선택에서 두 개의 행동이 있고 <img src="/rl-introduction-2nd-edit/tex/4f5b0033a3e7c19b1b1e947f7f15ba5a.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>라면 탐욕적 행동을 선택할 확률은 얼마인가?
> 행동이 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>이 있고, 최적 행동이 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>이라고 하자. <img src="/rl-introduction-2nd-edit/tex/4f5b0033a3e7c19b1b1e947f7f15ba5a.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>이기 때문에 <img src="/rl-introduction-2nd-edit/tex/3f19f7d8b2166a43a561f2a4f59dd203.svg?invert_in_darkmode&sanitize=true" align=middle width=78.89817704999999pt height=21.18721440000001pt/>이고 따라서 50% 확률로 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>을 선택하게 된다. 나머지 50% 선택은 가능한 모든 행동을 균등한 확률로(random) 두고 하게 된다. 따라서 <img src="/rl-introduction-2nd-edit/tex/56653dce53defc40d7f95771f2416fc2.svg?invert_in_darkmode&sanitize=true" align=middle width=108.67573529999999pt height=21.18721440000001pt/>, 즉 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>를 각각 25% 확률로 선택하게 된다. 정리해보면 최적 행동인 <img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>은 <img src="/rl-introduction-2nd-edit/tex/2e3765e074a4e2165a4f8c6675f1867f.svg?invert_in_darkmode&sanitize=true" align=middle width=121.46116949999997pt height=21.18721440000001pt/>, 75% 확률로 선택되며 <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>는 <img src="/rl-introduction-2nd-edit/tex/512128b1ead18bed472a510dafa27a24.svg?invert_in_darkmode&sanitize=true" align=middle width=29.22385289999999pt height=21.18721440000001pt/>, 25% 확률로 선택된다.

2. **(다중 선택 예제)** 네 개의 행동 중 하나를 선택하는 다중 선택 문제를 생각해 보자. 각 행동은 번호 1, 2, 3, 4로 구분한다.(<img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>, <img src="/rl-introduction-2nd-edit/tex/f87dcdc9f19dd4b14e54687dc1069783.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>) 이 문제에 입실론 탐욕적 행동 선택을 이용한 다중 선택 알고리즘을 적용한다고 생각해 보자. 이때 이 알고리즘은 표본 평균 행동 가치 추정값을 이용하고 초기 추정값 <img src="/rl-introduction-2nd-edit/tex/3fc01f9433855fb3c7801b5d10862d64.svg?invert_in_darkmode&sanitize=true" align=middle width=71.98131269999999pt height=24.65753399999998pt/>을 적용한다. 시간 단계에 따른 행동 및 가치의 몇몇 초기값들이 <img src="/rl-introduction-2nd-edit/tex/c4627d7c3e5d42229a5167cfc99140f6.svg?invert_in_darkmode&sanitize=true" align=middle width=69.01824929999998pt height=24.65753399999998pt/>a_{1}<img src="/rl-introduction-2nd-edit/tex/212b922b9e5eb33ea1d3651eaa804068.svg?invert_in_darkmode&sanitize=true" align=middle width=152.8009395pt height=24.65753399999998pt/>a_{2}<img src="/rl-introduction-2nd-edit/tex/a3fa16508b7ac1711e0aed8dbee1f4a3.svg?invert_in_darkmode&sanitize=true" align=middle width=140.01550695pt height=24.65753399999998pt/>a_{2}<img src="/rl-introduction-2nd-edit/tex/497237e8bb25706b263a5c8a8814f7ec.svg?invert_in_darkmode&sanitize=true" align=middle width=152.8009395pt height=24.65753399999998pt/>a_{2}<img src="/rl-introduction-2nd-edit/tex/591cc6d40b3d06c69a5e29f4e2e15439.svg?invert_in_darkmode&sanitize=true" align=middle width=140.01550695pt height=24.65753399999998pt/>a_{3}<img src="/rl-introduction-2nd-edit/tex/83ae3c3315f0970c07ceeac36347abe9.svg?invert_in_darkmode&sanitize=true" align=middle width=63.69137444999999pt height=24.65753399999998pt/>이라고 가정해보자. 이 시간 단계 중 일부에서 행동이 무작위로 선택되는 입실론 상황이 발행했을 수 있다. 어떤 시간 단계에서 이 상황이 확실하게 발생했을까? 어떤 시간 단계에서 이 상황이 발생하는 것이 가능했을까?

> 시간 순서대로 선택한 것을 기반으로 가치 테이블을 작성하면 아래와 같다.

|action|<img src="/rl-introduction-2nd-edit/tex/78e95e0d458b1115649366ef4e2feb10.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/cdf43ad0366b98bad8d28d8dc150f925.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/26f5cfaae3aa8ccd87fb3958011e4789.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/172c3f2c8c76d04bd95b98119aa143f3.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|<img src="/rl-introduction-2nd-edit/tex/065039dafe7712b36d84dc654fa11a5a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.48864374999999pt height=20.221802699999984pt/>|
|:-----|:-----:|:-----:|:-----:|:-----:|:-----:|
|<img src="/rl-introduction-2nd-edit/tex/079b75fd563cf05b47623e06b2003e64.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|-1    |-1     |-1     |-1     |-1     |
|<img src="/rl-introduction-2nd-edit/tex/764723ea3a0da0f66aaee1ae987f6abf.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|0     |1      |-0.5   |0.333  |0.333  |
|<img src="/rl-introduction-2nd-edit/tex/b3bed8ed07ed6311697ff7b39933375f.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|0     |0      |0      |0      |0      |
|<img src="/rl-introduction-2nd-edit/tex/f87dcdc9f19dd4b14e54687dc1069783.svg?invert_in_darkmode&sanitize=true" align=middle width=15.24170009999999pt height=14.15524440000002pt/>|0     |0      |0      |0      |0      |


3. 그림 2.2의 비교 그래프에서 `누적 보상`과 `최고의 행동을 선택할 확률`을 고려할 때 어떤 방법이 장기적으로 가장 좋은 성능을 보여줄 것인가? 얼마나 더 좋을 것인가? 이 문제에 대해 정량적으로 답변해 보라.
> 

4. 시간 간격의 크기 <img src="/rl-introduction-2nd-edit/tex/69533fe94ebdbe9cbed32f37b8365bb6.svg?invert_in_darkmode&sanitize=true" align=middle width=18.64167029999999pt height=14.15524440000002pt/>이 고정된 값이 아니라면 추정값 <img src="/rl-introduction-2nd-edit/tex/64d0afab00e7391a072599284b91840f.svg?invert_in_darkmode&sanitize=true" align=middle width=21.121448699999988pt height=22.465723500000017pt/>
은 이전까지 받은 보상들의 가중 평균이고, 이때 가중치는 식 2.6에서 주어지는 것과는 다르다. 식 2.6과 유사한 일반적인 경우에 있어서 바로 이전 보상에 적용할 가중치는 얼마인가? 시간 간격의 크기와 관련하여 답변해 보라.
* (식2.6) 

<p align="center"><img src="/rl-introduction-2nd-edit/tex/c11aebc10ad69cb5193d36f666a1fc61.svg?invert_in_darkmode&sanitize=true" align=middle width=353.8626531pt height=200.76626789999997pt/></p>

> 

5. *(programming)* 표본평균 방법을 비정상적(nonstationary) 문제에 적용하기 어렵다는 점을 보여주는 실험을 설계하고 수행하라. 모든 <img src="/rl-introduction-2nd-edit/tex/efc6578f07cd54c96cc674beb6d67c8a.svg?invert_in_darkmode&sanitize=true" align=middle width=36.36998804999999pt height=24.65753399999998pt/>가 동일한 초깃값으로부터 시작하며 독립적으로 무작위 값을 갖도록 변형한 10중 선택 문제를 활용하라(말하자면, 평균이 0이고 표준 편차가 0.01인 정규 분포를 따르는 확률 변수를 각 시간 단계에서 <img src="/rl-introduction-2nd-edit/tex/efc6578f07cd54c96cc674beb6d67c8a.svg?invert_in_darkmode&sanitize=true" align=middle width=36.36998804999999pt height=24.65753399999998pt/>에 더하도록 함으로써). 점증적으로 계산한 표본평균을 활용하는 행동 가치 방법과 고정된 시간 간격(<img src="/rl-introduction-2nd-edit/tex/6bc8e9f14104cfc2a848bdf86f2fca37.svg?invert_in_darkmode&sanitize=true" align=middle width=53.49877169999999pt height=21.18721440000001pt/>)을 사용하는 또 다른 행동 가치 방법에 대해 각각 그림 2.2와 같은 그래프를 그려 보라. <img src="/rl-introduction-2nd-edit/tex/946d41bc6d842a13d5592e2a97cc48fd.svg?invert_in_darkmode&sanitize=true" align=middle width=50.58777734999998pt height=21.18721440000001pt/>을 적용하고 더 많은 단계, 말하자면 10,000번의 단계를 적용해 보라.

> 

## Q&A