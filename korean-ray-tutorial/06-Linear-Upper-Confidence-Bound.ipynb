{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Linear Upper Confidence Bound\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous lesson](02-Simple-Multi-Armed-Bandit.ipynb), we used _LinUCB_ (Linear Upper Confidence Bound) for the exploration-explotation strategy ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb)), which assumes a linear dependency between the expected reward of an action and its context. \n",
    "\n",
    "Now we'll use _LinUCB_ in a recommendation environment with _parametric actions_, which are discrete actions that have continuous parameters. At each step, the agent must select which action to use and which parameters to use with that action. This increases the complexity of the context and the challenge of finding the optimal action to achieve the highest mean reward over time.\n",
    "\n",
    "See the previous discussion of UCB in [02 Exploration vs. Exploitation Strategies](02-Exploration-vs-Exploitation-Strategies.ipynb)  and the [previous lesson](03-Simple-Multi-Armed-Bandit.ipynb) .\n",
    "\n",
    "\n",
    "[previous lesson](02-Simple-Multi-Armed-Bandit.ipynb)에서는 exoloration-explotation 전략 [RLLIB documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb)에서 예상 보상간 선형 종속성을 가정하여 'LinUCB'(선형 상부 신뢰 경계)를 사용했다.그것은 어떤 행동의 예상 보상과 그 맥락 사이의 선형 종속성을 가정한다.\n",
    "\n",
    "이제 _parametric actions_ 가 있는 권고 환경에서 _LinUCB_ 를 사용할 것이며, 이는 연속적인 매개변수를 갖는 이산 작용이다. 각 단계에서 에이전트는 사용할 작업과 해당 작업에 사용할 매개 변수를 선택해야 한다. 이는 문맥의 복잡성과 시간에 따른 최고 평균 보상을 달성하기 위한 최적의 조치를 찾아야 하는 과제를 증가시킨다.\n",
    "\n",
    "UCB에 대한 이전 토론은 [02 Exploration vs.Exploitation](02-Exploration-vs-Exploitation-Strategies.ipynb)와 [previous lesson](03-단순-다중 암기-반딧.ipynb)에서 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray.rllib.contrib.bandits.agents.lin_ucb import UCB_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import ParametricItemRecoEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ParametricItemRecoEnv` ([parametric.py source code](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/parametric.py)) as the environment, which is a recommendation environment (\"RecoEnv\") that generates \"items\" (the \"parameters\") with randomly-generated features, some visible and some optionally hidden. The default sizes are governed by `DEFAULT_RECO_CONFIG` also in [parametric.py](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/parametric.py)):\n",
    "\n",
    "\n",
    "`ParametricItemRecoEnv`를 환경으로 사용한다.([parametric.py source code](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/parametric.py))\n",
    "이것은 임의로 생성된 특징과 가시성 및 선택적으로 숨겨져 있는 \"items\"(\"매개변수\")을 생성하는 권장 환경(\"RecoEnv\")이다.\n",
    "기본 크기는 `DEFAULT_RECO_CONFIG` [parametric.py](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/parametric.py))에 의해 \n",
    "변한다. \n",
    "\n",
    "```python\n",
    "DEFAULT_RECO_CONFIG = {\n",
    "    \"num_users\": 1,        # More than one user at a time?\n",
    "    \"num_items\": 100,      # Number of items to randomly sample.\n",
    "    \"feature_dim\": 16,     # Number of features per item, with randomly generated values\n",
    "    \"slate_size\": 1,       # More than one step at a time?\n",
    "    \"num_candidates\": 25,  # Determines the action space and the the number of items randomly sampled from the num_items items.\n",
    "    \"seed\": 1              # For randomization\n",
    "}\n",
    "```\n",
    "\n",
    "This environment is deliberately complicated, so it is nontrivial, but that means it is confusing to understand at first. So, let's look at its behavior. We'll create one using the default settings:\n",
    "\n",
    "이런 환경은 의도적으로 복잡하기 때문에 비교가 안 되지만, 그것은 처음에는 이해하기가 혼란스럽다는 것을 의미한다. 자, 이것의 행동을 살펴봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: Discrete(25) (number of actions that can be selected)\n"
     ]
    }
   ],
   "source": [
    "pire = ParametricItemRecoEnv()\n",
    "pire.reset()\n",
    "print(f'action space: {pire.action_space} (number of actions that can be selected)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step():\n",
    "    action = pire.action_space.sample()\n",
    "    obs, reward, finished, info = pire.step(action)\n",
    "    obs_item_foo = f\"{obs['item'][:1]} ({len(obs['item'])} items)\"\n",
    "    print(f\"\"\"\n",
    "    action = {action}, \n",
    "    obs:\n",
    "        'item': {obs_item_foo}, \n",
    "        'item_id': {obs['item_id']},\n",
    "        'response': {obs['response']}, \n",
    "    reward = {reward}, \n",
    "    finished? = {finished}, \n",
    "    info = {info}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    action = 13, \n",
      "    obs:\n",
      "        'item': [[0.02184632 0.38962268 0.27080361 0.06139751 0.16461075 0.24289644\n",
      "  0.27257102 0.21258848 0.00558852 0.22869068 0.12475031 0.31299057\n",
      "  0.24970903 0.38632188 0.39342137 0.18108697]] (25 items), \n",
      "        'item_id': [85 46 88 35 76 22  8 44 49 92 31 41 28 33 84 61  2 43 47 39 81 38 29 34\n",
      " 64],\n",
      "        'response': [0.69418285042524], \n",
      "    reward = 0.69418285042524, \n",
      "    finished? = True, \n",
      "    info = {'regret': 0.17517313859435446}\n",
      "    \n",
      "\n",
      "    action = 8, \n",
      "    obs:\n",
      "        'item': [[0.28213117 0.26317668 0.21036557 0.37617669 0.00369184 0.04024035\n",
      "  0.05232339 0.08387468 0.32555471 0.05242273 0.35259738 0.34257478\n",
      "  0.04837945 0.37538231 0.23187539 0.32639976]] (25 items), \n",
      "        'item_id': [24 65 44 30 59 95 16 86 78 22 48 28 71 35 46 90 92 31 10  0 18 82 12 43\n",
      " 62],\n",
      "        'response': [0.7414915799974681], \n",
      "    reward = 0.7414915799974681, \n",
      "    finished? = True, \n",
      "    info = {'regret': 0.12786440902212637}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "take_step()\n",
    "take_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you see a warning about _Box bound precision lowered by casting to float32_, you can safely ignore it.\n",
    "\n",
    "The rewards at each step are randomly computed using matrix multiplication of the various randomly-generated matrices of data, followed by selecting a response (reward), indexed by the particular action specified to `step`. However, as constructed the reward always comes out between about 0.6 and 0.9 and the regret is the maximum value over all possible actions minus the reward for the specified action. \n",
    "\n",
    "The `item` shown is the subset of all the _items_ in the environment, with the `item_id` being the corresponding indices of the items shown in the larger collection of items. This list of 25 items is randomly chosen _for each step_, as you should be able to see from these two steps.\n",
    "\n",
    "각 단계의 보상은 무작위로 생성된 다양한 데이터 행렬의 행렬 곱셈을 사용하여 무작위로 계산한 다음, '단계'에 지정된 특정 동작에 의해 색인화된 반응(보상)을 선택한다. 그러나 구성된 보상금은 항상 약 0.6에서 0.9 사이에 나오며, 후회하는 것은 가능한 모든 행동에 대한 최대 값에서 지정된 조치에 대한 보상을 뺀 값이다.\n",
    "\n",
    "표시된 `item`은 환경에 있는 모든 _항목의 하위 집합이며, `item_id`는 더 큰 항목 집합에 나타난 항목의 해당 지표가 된다. 이 25개 항목의 목록은 이 두 단계에서 볼 수 있듯이 각 단계별로 임의로 선택된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following `num_candidates` steps, which defaults to 25, you may see one regret of 0.0, which happens to be when the action was selected with the maximum possible reward, but not for all runs. Which one has the lowest regret?\n",
    "\n",
    "기본값이 25인 다음의 `num_pandidates` 단계에서는 0.0의 한 가지 후회를 볼 수 있는데, 이는 모두 대해서는 아니지만 최대한의 보상을 받고 액션을 선택했을 때의 일이다. 어느 것이 가장 후회가 적은가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: reward = 0.79091, regret = 0.07845\n",
      "  1: reward = 0.77886, regret = 0.08691\n",
      "  2: reward = 0.72152, regret = 0.14784\n",
      "  3: reward = 0.64857, regret = 0.21899\n",
      "  4: reward = 0.70884, regret = 0.12224\n",
      "  5: reward = 0.59567, regret = 0.21234\n",
      "  6: reward = 0.82330, regret = 0.03029\n",
      "  7: reward = 0.65356, regret = 0.21221\n",
      "  8: reward = 0.72152, regret = 0.14425\n",
      "  9: reward = 0.84557, regret = 0.02378\n",
      " 10: reward = 0.76898, regret = 0.10037\n",
      " 11: reward = 0.76763, regret = 0.04319\n",
      " 12: reward = 0.70214, regret = 0.12894\n",
      " 13: reward = 0.70325, regret = 0.16966\n",
      " 14: reward = 0.68384, regret = 0.16173\n",
      " 15: reward = 0.82330, regret = 0.04962\n",
      " 16: reward = 0.68384, regret = 0.18193\n",
      " 17: reward = 0.78811, regret = 0.06548\n",
      " 18: reward = 0.68217, regret = 0.17142\n",
      " 19: reward = 0.53511, regret = 0.31848\n",
      " 20: reward = 0.69312, regret = 0.16047\n",
      " 21: reward = 0.75453, regret = 0.09906\n",
      " 22: reward = 0.83108, regret = 0.00000\n",
      " 23: reward = 0.78036, regret = 0.08719\n",
      " 24: reward = 0.78497, regret = 0.07180\n"
     ]
    }
   ],
   "source": [
    "for i in range(pire.num_candidates):\n",
    "    action = pire.action_space.sample()\n",
    "    obs, reward, finished, info = pire.step(action)\n",
    "    print(f'{i:3d}: reward = {reward:7.5f}, regret = {info[\"regret\"]:7.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The up shot is that training to find the optimal, mean reward will be more challenging than our previous simple bandit.\n",
    "\n",
    "Now that we've explored `ParametricItemRecoEnv`, let's use it with _LinUCB_.\n",
    "\n",
    "Note that we imported `UCB_CONFIG` above, which has the properties defined that are expected _LinUCB_. We'll add another property to it for the environment. (Subsequent lessons will show other ways to work with the configuration.)\n",
    "\n",
    "이전의 단순한 bandit보다 더 나은, 평균 보상을 찾기 위한 훈련이 더 어려울 것이다.\n",
    "\n",
    "이제 `ParametricItemRecoEnv`를 살펴보았으니 _LinUCB_와 함께 사용해 보자.\n",
    "\n",
    "상기 `UCB_CONFIG`를 import했는데, 이 속성은 _LinUCB_ 로 정의되어 있다. 환경을 위해 또 다른 속성을 추가하겠다. (후속적인 노트북들은 구성을 사용하는 다른 방법을 보여줄 것이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for 20 time steps\n"
     ]
    }
   ],
   "source": [
    "UCB_CONFIG[\"env\"] = ParametricItemRecoEnv\n",
    "\n",
    "# Actual training_iterations will be 20 * timesteps_per_iteration (100 by default) = 2,000\n",
    "training_iterations = 20\n",
    "\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use [Ray Tune](http://tune.io) to train. First start Ray or connect to a running cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-12 16:49:27,974\tINFO resource_spec.py:231 -- Starting Ray with 3.42 GiB memory available for workers and up to 1.71 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-09-12 16:49:28,535\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.5',\n",
       " 'raylet_ip_address': '192.168.1.5',\n",
       " 'redis_address': '192.168.1.5:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-09-12_16-49-27_972797_31350/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-09-12_16-49-27_972797_31350/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-09-12_16-49-27_972797_31350'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.2/7.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.42 GiB heap, 0.0/1.17 GiB objects<br>Result logdir: /home/jhmbabo/ray_results/contrib/LinUCB<br>Number of trials: 5 (5 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_7dc80_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        10.1914 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.87834 </td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_7dc80_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         9.97493</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.889897</td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_7dc80_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        10.1432 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.84754 </td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_7dc80_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        10.352  </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.893462</td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_7dc80_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.20884</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.878567</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis = ray.tune.run(\n",
    "    \"contrib/LinUCB\",\n",
    "    config=UCB_CONFIG,\n",
    "    stop={\"training_iteration\": training_iterations},\n",
    "    num_samples=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23.19 seconds,    0.39 minutes\n"
     ]
    }
   ],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>config/sample_batch_size</th>\n",
       "      <th>config/seed</th>\n",
       "      <th>config/shuffle_buffer_size</th>\n",
       "      <th>config/soft_horizon</th>\n",
       "      <th>config/synchronize_filters</th>\n",
       "      <th>config/tf_session_args</th>\n",
       "      <th>config/timesteps_per_iteration</th>\n",
       "      <th>config/train_batch_size</th>\n",
       "      <th>config/use_pytorch</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.914414</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.878340</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jhmbabo/ray_results/contrib/LinUCB/contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.898686</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.889897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jhmbabo/ray_results/contrib/LinUCB/contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.901036</td>\n",
       "      <td>0.777299</td>\n",
       "      <td>0.847540</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jhmbabo/ray_results/contrib/LinUCB/contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.904757</td>\n",
       "      <td>0.825116</td>\n",
       "      <td>0.893462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jhmbabo/ray_results/contrib/LinUCB/contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.892011</td>\n",
       "      <td>0.819377</td>\n",
       "      <td>0.878567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jhmbabo/ray_results/contrib/LinUCB/contr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0            0.914414            0.808219             0.878340   \n",
       "1            0.898686            0.848535             0.889897   \n",
       "2            0.901036            0.777299             0.847540   \n",
       "3            0.904757            0.825116             0.893462   \n",
       "4            0.892011            0.819377             0.878567   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0               1.0                 100                    0             2000   \n",
       "1               1.0                 100                    0             2000   \n",
       "2               1.0                 100                    0             2000   \n",
       "3               1.0                 100                    0             2000   \n",
       "4               1.0                 100                    0             2000   \n",
       "\n",
       "   done  episodes_total  training_iteration  ... config/sample_batch_size  \\\n",
       "0  True            2000                  20  ...                       -1   \n",
       "1  True            2000                  20  ...                       -1   \n",
       "2  True            2000                  20  ...                       -1   \n",
       "3  True            2000                  20  ...                       -1   \n",
       "4  True            2000                  20  ...                       -1   \n",
       "\n",
       "  config/seed  config/shuffle_buffer_size  config/soft_horizon  \\\n",
       "0        None                           0                False   \n",
       "1        None                           0                False   \n",
       "2        None                           0                False   \n",
       "3        None                           0                False   \n",
       "4        None                           0                False   \n",
       "\n",
       "   config/synchronize_filters  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                              config/tf_session_args  \\\n",
       "0  {'allow_soft_placement': True, 'device_count':...   \n",
       "1  {'allow_soft_placement': True, 'device_count':...   \n",
       "2  {'allow_soft_placement': True, 'device_count':...   \n",
       "3  {'allow_soft_placement': True, 'device_count':...   \n",
       "4  {'allow_soft_placement': True, 'device_count':...   \n",
       "\n",
       "  config/timesteps_per_iteration config/train_batch_size  config/use_pytorch  \\\n",
       "0                            100                       1                  -1   \n",
       "1                            100                       1                  -1   \n",
       "2                            100                       1                  -1   \n",
       "3                            100                       1                  -1   \n",
       "4                            100                       1                  -1   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/jhmbabo/ray_results/contrib/LinUCB/contr...  \n",
       "1  /home/jhmbabo/ray_results/contrib/LinUCB/contr...  \n",
       "2  /home/jhmbabo/ray_results/contrib/LinUCB/contr...  \n",
       "3  /home/jhmbabo/ray_results/contrib/LinUCB/contr...  \n",
       "4  /home/jhmbabo/ray_results/contrib/LinUCB/contr...  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `episode_reward_mean` values. Now let's analyze the _cumulative regrets_ of the trials. It's inevitable that we sometimes pick a suboptimal action, but was this done less often as time progressed?\n",
    "\n",
    "`episode_reward_mean` 값을 기록해 두십시오. 이제 그 시도의 _cumulative regrets_ 를 분석해 보자. 어쩔 수 없이 차선책을 택하기도 하지만 시간이 지날수록 이런 일이 줄어들지 않았을까."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the columns in the trial dataframes is the `info/learner/default_policy/cumulative_regret`. Let's combine the trail DataFrames into a single DataFrame, then group over the `info/number_steps_trained` and project out the `info/learner/default_policy/cumulative_regret`. Finally, aggregate for each `info/number_steps_trained` to compute the `mean`, `max`, `min`, and `std` (standard deviation) for the cumulative regret.\n",
    "\n",
    "\n",
    "평가 데이터 프레임의 열 중 하나는 `info/learner/default_policy/cumulative_regret`이다. 데이터프레임을 단일 DataFrame으로 결합한 다음, `info/number_step_learned`에 그룹화하고 `info/learner/default_policy/cumulation_reregret`를 투영해 봅시다. 마지막으로, 훈련된 각 `info/number_steps_trained`을 집계하여 `mean`, `max`, `min`, `std`(표준편차)를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame()\n",
    "\n",
    "for key, df in analysis.trial_dataframes.items():\n",
    "    frame = frame.append(df, ignore_index=True)\n",
    "\n",
    "df = frame.groupby(\"info/num_steps_trained\")[\n",
    "    \"info/learner/default_policy/cumulative_regret\"].aggregate([\"mean\", \"max\", \"min\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>info/num_steps_trained</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.222065</td>\n",
       "      <td>3.409440</td>\n",
       "      <td>3.060718</td>\n",
       "      <td>0.142948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.917847</td>\n",
       "      <td>4.288206</td>\n",
       "      <td>3.731046</td>\n",
       "      <td>0.226175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.280946</td>\n",
       "      <td>4.724587</td>\n",
       "      <td>3.977982</td>\n",
       "      <td>0.312146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.551197</td>\n",
       "      <td>5.166770</td>\n",
       "      <td>4.098278</td>\n",
       "      <td>0.442226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.724890</td>\n",
       "      <td>5.374023</td>\n",
       "      <td>4.209122</td>\n",
       "      <td>0.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.902917</td>\n",
       "      <td>5.538157</td>\n",
       "      <td>4.335885</td>\n",
       "      <td>0.523566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.002488</td>\n",
       "      <td>5.671459</td>\n",
       "      <td>4.364087</td>\n",
       "      <td>0.597292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.115244</td>\n",
       "      <td>5.792358</td>\n",
       "      <td>4.392140</td>\n",
       "      <td>0.628825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>5.208474</td>\n",
       "      <td>5.977497</td>\n",
       "      <td>4.431327</td>\n",
       "      <td>0.678480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.275504</td>\n",
       "      <td>6.086007</td>\n",
       "      <td>4.468954</td>\n",
       "      <td>0.703333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>5.337236</td>\n",
       "      <td>6.186615</td>\n",
       "      <td>4.490556</td>\n",
       "      <td>0.734319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>5.413999</td>\n",
       "      <td>6.267735</td>\n",
       "      <td>4.513879</td>\n",
       "      <td>0.754621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>5.482734</td>\n",
       "      <td>6.368316</td>\n",
       "      <td>4.543575</td>\n",
       "      <td>0.769032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>5.533466</td>\n",
       "      <td>6.452487</td>\n",
       "      <td>4.554422</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.595057</td>\n",
       "      <td>6.516930</td>\n",
       "      <td>4.586683</td>\n",
       "      <td>0.839229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>5.652957</td>\n",
       "      <td>6.579255</td>\n",
       "      <td>4.592825</td>\n",
       "      <td>0.850709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>5.702130</td>\n",
       "      <td>6.647555</td>\n",
       "      <td>4.612854</td>\n",
       "      <td>0.876918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>5.738712</td>\n",
       "      <td>6.713075</td>\n",
       "      <td>4.615173</td>\n",
       "      <td>0.889905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>5.777827</td>\n",
       "      <td>6.780251</td>\n",
       "      <td>4.618447</td>\n",
       "      <td>0.910414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.811500</td>\n",
       "      <td>6.828343</td>\n",
       "      <td>4.634021</td>\n",
       "      <td>0.913810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mean       max       min       std\n",
       "info/num_steps_trained                                        \n",
       "100                     3.222065  3.409440  3.060718  0.142948\n",
       "200                     3.917847  4.288206  3.731046  0.226175\n",
       "300                     4.280946  4.724587  3.977982  0.312146\n",
       "400                     4.551197  5.166770  4.098278  0.442226\n",
       "500                     4.724890  5.374023  4.209122  0.483900\n",
       "600                     4.902917  5.538157  4.335885  0.523566\n",
       "700                     5.002488  5.671459  4.364087  0.597292\n",
       "800                     5.115244  5.792358  4.392140  0.628825\n",
       "900                     5.208474  5.977497  4.431327  0.678480\n",
       "1000                    5.275504  6.086007  4.468954  0.703333\n",
       "1100                    5.337236  6.186615  4.490556  0.734319\n",
       "1200                    5.413999  6.267735  4.513879  0.754621\n",
       "1300                    5.482734  6.368316  4.543575  0.769032\n",
       "1400                    5.533466  6.452487  4.554422  0.820985\n",
       "1500                    5.595057  6.516930  4.586683  0.839229\n",
       "1600                    5.652957  6.579255  4.592825  0.850709\n",
       "1700                    5.702130  6.647555  4.612854  0.876918\n",
       "1800                    5.738712  6.713075  4.615173  0.889905\n",
       "1900                    5.777827  6.780251  4.618447  0.910414\n",
       "2000                    5.811500  6.828343  4.634021  0.913810"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to understand these results with a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-518bf1d8a494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_cumulative_regret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# The next two lines prevent Bokeh from opening the graph in a new window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh_util'"
     ]
    }
   ],
   "source": [
    "from bokeh_util import plot_cumulative_regret\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "import bokeh\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_cumulative_regret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f8b8828db9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_cumulative_regret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_cumulative_regret' is not defined"
     ]
    }
   ],
   "source": [
    "plot_cumulative_regret(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../images/rllib/LinUCB-Cumulative-Regret.png))\n",
    "\n",
    "So the _cumulative_ regret increases for the entire number of training steps for all five trials, but for larger step numbers, the amount of regret added decreases as we learn, so the graph begins to level off as the system gets better at optimizing the mean reward.\n",
    "\n",
    "따라서 _누적_ 후회는 다섯 번의 모든 시험에서 전체 훈련 단계 수에 대해 증가하지만, 더 큰 단계 숫자의 경우, 우리가 배울수록 후회의 추가 양이 감소하기 때문에, 시스템이 평균 보상을 최적화함에 따라 그래프가 평준화되기 시작한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment we're using randomly generates data on every step, so there will always be some regret even if we train for a longer period of time.\n",
    "\n",
    "우리가 사용하는 환경은 모든 단계에서 무작위로 데이터를 생성하기 때문에 더 긴 시간 훈련을 하더라도 항상 약간의 후회가 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the `training_iterations` from 20 to 40. Does the characteristic behavior of cumulative regret change at higher steps?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
