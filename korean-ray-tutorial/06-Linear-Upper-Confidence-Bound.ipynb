{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Linear Upper Confidence Bound\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous lesson](02-Simple-Multi-Armed-Bandit.ipynb), we used _LinUCB_ (Linear Upper Confidence Bound) for the exploration-explotation strategy ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb)), which assumes a linear dependency between the expected reward of an action and its context. \n",
    "\n",
    "Now we'll use _LinUCB_ in a recommendation environment with _parametric actions_, which are discrete actions that have continuous parameters. At each step, the agent must select which action to use and which parameters to use with that action. This increases the complexity of the context and the challenge of finding the optimal action to achieve the highest mean reward over time.\n",
    "\n",
    "See the previous discussion of UCB in [02 Exploration vs. Exploitation Strategies](02-Exploration-vs-Exploitation-Strategies.ipynb)  and the [previous lesson](03-Simple-Multi-Armed-Bandit.ipynb) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jungyeon/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray.rllib.contrib.bandits.agents.lin_ucb import UCB_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import ParametricItemRecoEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ParametricItemRecoEnv` ([parametric.py source code](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/parametric.py)) as the environment, which is a recommendation environment (\"RecoEnv\") that generates \"items\" (the \"parameters\") with randomly-generated features, some visible and some optionally hidden. The default sizes are governed by `DEFAULT_RECO_CONFIG` also in [parametric.py](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/parametric.py)):\n",
    "\n",
    "```python\n",
    "DEFAULT_RECO_CONFIG = {\n",
    "    \"num_users\": 1,        # More than one user at a time?\n",
    "    \"num_items\": 100,      # Number of items to randomly sample.\n",
    "    \"feature_dim\": 16,     # Number of features per item, with randomly generated values\n",
    "    \"slate_size\": 1,       # More than one step at a time?\n",
    "    \"num_candidates\": 25,  # Determines the action space and the the number of items randomly sampled from the num_items items.\n",
    "    \"seed\": 1              # For randomization\n",
    "}\n",
    "```\n",
    "\n",
    "This environment is deliberately complicated, so it is nontrivial, but that means it is confusing to understand at first. So, let's look at its behavior. We'll create one using the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: Discrete(25) (number of actions that can be selected)\n"
     ]
    }
   ],
   "source": [
    "pire = ParametricItemRecoEnv()\n",
    "pire.reset()\n",
    "print(f'action space: {pire.action_space} (number of actions that can be selected)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step():\n",
    "    action = pire.action_space.sample()\n",
    "    obs, reward, finished, info = pire.step(action)\n",
    "    obs_item_foo = f\"{obs['item'][:1]} ({len(obs['item'])} items)\"\n",
    "    print(f\"\"\"\n",
    "    action = {action}, \n",
    "    obs:\n",
    "        'item': {obs_item_foo}, \n",
    "        'item_id': {obs['item_id']},\n",
    "        'response': {obs['response']}, \n",
    "    reward = {reward}, \n",
    "    finished? = {finished}, \n",
    "    info = {info}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    action = 10, \n",
      "    obs:\n",
      "        'item': [[0.05048774 0.47194573 0.03921339 0.30052419 0.20338461 0.04726611\n",
      "  0.12836221 0.07173992 0.22054862 0.1293311  0.10709322 0.30443624\n",
      "  0.41910891 0.14934622 0.16536158 0.47204157]] (25 items), \n",
      "        'item_id': [50 53 39 90 40 54 70 11 85 21 42 35 48 55 74 49 51 60 92 68 62 29 81 72\n",
      " 93],\n",
      "        'response': [0.8220774924228555], \n",
      "    reward = 0.8220774924228555, \n",
      "    finished? = True, \n",
      "    info = {'regret': 0.10491187669598778}\n",
      "    \n",
      "\n",
      "    action = 6, \n",
      "    obs:\n",
      "        'item': [[0.31548275 0.11021091 0.353385   0.23614226 0.39811143 0.42167002\n",
      "  0.27204124 0.15484602 0.19972821 0.14739303 0.10179081 0.13443356\n",
      "  0.22163083 0.3115957  0.18870421 0.03907462]] (25 items), \n",
      "        'item_id': [16 91  8 59 41 64 92  0 57 99 66 26 84 35 19 49 72 83 27 88  1  4 50 65\n",
      " 85],\n",
      "        'response': [0.7968325805543849], \n",
      "    reward = 0.7968325805543849, \n",
      "    finished? = True, \n",
      "    info = {'regret': 0.13711410502985222}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "take_step()\n",
    "take_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you see a warning about _Box bound precision lowered by casting to float32_, you can safely ignore it.\n",
    "\n",
    "The rewards at each step are randomly computed using matrix multiplication of the various randomly-generated matrices of data, followed by selecting a response (reward), indexed by the particular action specified to `step`. However, as constructed the reward always comes out between about 0.6 and 0.9 and the regret is the maximum value over all possible actions minus the reward for the specified action. \n",
    "\n",
    "The `item` shown is the subset of all the _items_ in the environment, with the `item_id` being the corresponding indices of the items shown in the larger collection of items. This list of 25 items is randomly chosen _for each step_, as you should be able to see from these two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following `num_candidates` steps, which defaults to 25, you may see one regret of 0.0, which happens to be when the action was selected with the maximum possible reward, but not for all runs. Which one has the lowest regret?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: reward = 0.83539, regret = 0.06087\n",
      "  1: reward = 0.77759, regret = 0.13725\n",
      "  2: reward = 0.83032, regret = 0.02600\n",
      "  3: reward = 0.83455, regret = 0.10204\n",
      "  4: reward = 0.82078, regret = 0.11317\n",
      "  5: reward = 0.83310, regret = 0.10349\n",
      "  6: reward = 0.79832, regret = 0.13563\n",
      "  7: reward = 0.81241, regret = 0.09064\n",
      "  8: reward = 0.79307, regret = 0.14352\n",
      "  9: reward = 0.79832, regret = 0.13827\n",
      " 10: reward = 0.81913, regret = 0.10786\n",
      " 11: reward = 0.70258, regret = 0.22441\n",
      " 12: reward = 0.76002, regret = 0.17657\n",
      " 13: reward = 0.82647, regret = 0.07658\n",
      " 14: reward = 0.79832, regret = 0.11652\n",
      " 15: reward = 0.73610, regret = 0.17874\n",
      " 16: reward = 0.83455, regret = 0.10204\n",
      " 17: reward = 0.83831, regret = 0.08868\n",
      " 18: reward = 0.83032, regret = 0.06594\n",
      " 19: reward = 0.85926, regret = 0.04379\n",
      " 20: reward = 0.85031, regret = 0.07668\n",
      " 21: reward = 0.73631, regret = 0.20028\n",
      " 22: reward = 0.91484, regret = 0.00000\n",
      " 23: reward = 0.79832, regret = 0.09794\n",
      " 24: reward = 0.78102, regret = 0.13381\n"
     ]
    }
   ],
   "source": [
    "for i in range(pire.num_candidates):\n",
    "    action = pire.action_space.sample()\n",
    "    obs, reward, finished, info = pire.step(action)\n",
    "    print(f'{i:3d}: reward = {reward:7.5f}, regret = {info[\"regret\"]:7.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The up shot is that training to find the optimal, mean reward will be more challenging than our previous simple bandit.\n",
    "\n",
    "Now that we've explored `ParametricItemRecoEnv`, let's use it with _LinUCB_.\n",
    "\n",
    "Note that we imported `UCB_CONFIG` above, which has the properties defined that are expected _LinUCB_. We'll add another property to it for the environment. (Subsequent lessons will show other ways to work with the configuration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for 20 time steps\n"
     ]
    }
   ],
   "source": [
    "UCB_CONFIG[\"env\"] = ParametricItemRecoEnv\n",
    "\n",
    "# Actual training_iterations will be 20 * timesteps_per_iteration (100 by default) = 2,000\n",
    "training_iterations = 20\n",
    "\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use [Ray Tune](http://tune.io) to train. First start Ray or connect to a running cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 04:04:52,133\tINFO resource_spec.py:231 -- Starting Ray with 8.06 GiB memory available for workers and up to 4.05 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-09-01 04:04:52,652\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.1.1',\n",
       " 'raylet_ip_address': '127.0.1.1',\n",
       " 'redis_address': '127.0.1.1:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-09-01_04-04-52_131638_4976/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-09-01_04-04-52_131638_4976/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-09-01_04-04-52_131638_4976'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/8.06 GiB heap, 0.0/2.78 GiB objects<br>Result logdir: /home/jungyeon/ray_results/contrib/LinUCB<br>Number of trials: 5 (5 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_da6f0_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.79818</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.899049</td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_da6f0_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.72762</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.868243</td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_da6f0_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.74713</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.898301</td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_da6f0_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.61002</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.89894 </td></tr>\n",
       "<tr><td>contrib_LinUCB_ParametricItemRecoEnv_da6f0_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5.40985</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.862724</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis = ray.tune.run(\n",
    "    \"contrib/LinUCB\",\n",
    "    config=UCB_CONFIG,\n",
    "    stop={\"training_iteration\": training_iterations},\n",
    "    num_samples=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9.74 seconds,    0.16 minutes\n"
     ]
    }
   ],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>config/sample_batch_size</th>\n",
       "      <th>config/seed</th>\n",
       "      <th>config/shuffle_buffer_size</th>\n",
       "      <th>config/soft_horizon</th>\n",
       "      <th>config/synchronize_filters</th>\n",
       "      <th>config/tf_session_args</th>\n",
       "      <th>config/timesteps_per_iteration</th>\n",
       "      <th>config/train_batch_size</th>\n",
       "      <th>config/use_pytorch</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.909727</td>\n",
       "      <td>0.841158</td>\n",
       "      <td>0.899049</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jungyeon/ray_results/contrib/LinUCB/cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.910887</td>\n",
       "      <td>0.804417</td>\n",
       "      <td>0.868243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jungyeon/ray_results/contrib/LinUCB/cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.929446</td>\n",
       "      <td>0.836176</td>\n",
       "      <td>0.898301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jungyeon/ray_results/contrib/LinUCB/cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.923361</td>\n",
       "      <td>0.852523</td>\n",
       "      <td>0.898940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jungyeon/ray_results/contrib/LinUCB/cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.886507</td>\n",
       "      <td>0.803067</td>\n",
       "      <td>0.862724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'allow_soft_placement': True, 'device_count':...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>/home/jungyeon/ray_results/contrib/LinUCB/cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0            0.909727            0.841158             0.899049   \n",
       "1            0.910887            0.804417             0.868243   \n",
       "2            0.929446            0.836176             0.898301   \n",
       "3            0.923361            0.852523             0.898940   \n",
       "4            0.886507            0.803067             0.862724   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0               1.0                 100                    0             2000   \n",
       "1               1.0                 100                    0             2000   \n",
       "2               1.0                 100                    0             2000   \n",
       "3               1.0                 100                    0             2000   \n",
       "4               1.0                 100                    0             2000   \n",
       "\n",
       "   done  episodes_total  training_iteration  ... config/sample_batch_size  \\\n",
       "0  True            2000                  20  ...                       -1   \n",
       "1  True            2000                  20  ...                       -1   \n",
       "2  True            2000                  20  ...                       -1   \n",
       "3  True            2000                  20  ...                       -1   \n",
       "4  True            2000                  20  ...                       -1   \n",
       "\n",
       "  config/seed  config/shuffle_buffer_size  config/soft_horizon  \\\n",
       "0        None                           0                False   \n",
       "1        None                           0                False   \n",
       "2        None                           0                False   \n",
       "3        None                           0                False   \n",
       "4        None                           0                False   \n",
       "\n",
       "   config/synchronize_filters  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                              config/tf_session_args  \\\n",
       "0  {'allow_soft_placement': True, 'device_count':...   \n",
       "1  {'allow_soft_placement': True, 'device_count':...   \n",
       "2  {'allow_soft_placement': True, 'device_count':...   \n",
       "3  {'allow_soft_placement': True, 'device_count':...   \n",
       "4  {'allow_soft_placement': True, 'device_count':...   \n",
       "\n",
       "  config/timesteps_per_iteration config/train_batch_size  config/use_pytorch  \\\n",
       "0                            100                       1                  -1   \n",
       "1                            100                       1                  -1   \n",
       "2                            100                       1                  -1   \n",
       "3                            100                       1                  -1   \n",
       "4                            100                       1                  -1   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/jungyeon/ray_results/contrib/LinUCB/cont...  \n",
       "1  /home/jungyeon/ray_results/contrib/LinUCB/cont...  \n",
       "2  /home/jungyeon/ray_results/contrib/LinUCB/cont...  \n",
       "3  /home/jungyeon/ray_results/contrib/LinUCB/cont...  \n",
       "4  /home/jungyeon/ray_results/contrib/LinUCB/cont...  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `episode_reward_mean` values. Now let's analyze the _cumulative regrets_ of the trials. It's inevitable that we sometimes pick a suboptimal action, but was this done less often as time progressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the columns in the trial dataframes is the `info/learner/default_policy/cumulative_regret`. Let's combine the trail DataFrames into a single DataFrame, then group over the `info/number_steps_trained` and project out the `info/learner/default_policy/cumulative_regret`. Finally, aggregate for each `info/number_steps_trained` to compute the `mean`, `max`, `min`, and `std` (standard deviation) for the cumulative regret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame()\n",
    "\n",
    "for key, df in analysis.trial_dataframes.items():\n",
    "    frame = frame.append(df, ignore_index=True)\n",
    "\n",
    "df = frame.groupby(\"info/num_steps_trained\")[\n",
    "    \"info/learner/default_policy/cumulative_regret\"].aggregate([\"mean\", \"max\", \"min\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>info/num_steps_trained</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.387118</td>\n",
       "      <td>3.837414</td>\n",
       "      <td>3.186883</td>\n",
       "      <td>0.257097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>3.997620</td>\n",
       "      <td>4.355994</td>\n",
       "      <td>3.793556</td>\n",
       "      <td>0.224908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>4.336677</td>\n",
       "      <td>4.524341</td>\n",
       "      <td>4.104159</td>\n",
       "      <td>0.207311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>4.526428</td>\n",
       "      <td>4.832583</td>\n",
       "      <td>4.236853</td>\n",
       "      <td>0.276685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>4.776828</td>\n",
       "      <td>5.174206</td>\n",
       "      <td>4.431874</td>\n",
       "      <td>0.353029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>4.981517</td>\n",
       "      <td>5.533187</td>\n",
       "      <td>4.572223</td>\n",
       "      <td>0.446839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>5.079321</td>\n",
       "      <td>5.690763</td>\n",
       "      <td>4.668314</td>\n",
       "      <td>0.482136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>5.214527</td>\n",
       "      <td>5.876961</td>\n",
       "      <td>4.755728</td>\n",
       "      <td>0.515497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>5.328502</td>\n",
       "      <td>5.967244</td>\n",
       "      <td>4.857674</td>\n",
       "      <td>0.542761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>5.417954</td>\n",
       "      <td>6.052870</td>\n",
       "      <td>4.922895</td>\n",
       "      <td>0.565076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>5.482271</td>\n",
       "      <td>6.166809</td>\n",
       "      <td>4.935299</td>\n",
       "      <td>0.603142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>5.530989</td>\n",
       "      <td>6.228631</td>\n",
       "      <td>4.990440</td>\n",
       "      <td>0.610849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>5.568979</td>\n",
       "      <td>6.273621</td>\n",
       "      <td>4.995877</td>\n",
       "      <td>0.634239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>5.622214</td>\n",
       "      <td>6.365644</td>\n",
       "      <td>5.037643</td>\n",
       "      <td>0.656974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>5.702749</td>\n",
       "      <td>6.498899</td>\n",
       "      <td>5.143312</td>\n",
       "      <td>0.664541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>5.751283</td>\n",
       "      <td>6.590142</td>\n",
       "      <td>5.191261</td>\n",
       "      <td>0.680211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>5.807898</td>\n",
       "      <td>6.713574</td>\n",
       "      <td>5.233725</td>\n",
       "      <td>0.702568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>5.850186</td>\n",
       "      <td>6.757181</td>\n",
       "      <td>5.280823</td>\n",
       "      <td>0.698126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>5.906923</td>\n",
       "      <td>6.790075</td>\n",
       "      <td>5.333257</td>\n",
       "      <td>0.685455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>5.946962</td>\n",
       "      <td>6.843815</td>\n",
       "      <td>5.336309</td>\n",
       "      <td>0.699718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mean       max       min       std\n",
       "info/num_steps_trained                                        \n",
       "100                     3.387118  3.837414  3.186883  0.257097\n",
       "200                     3.997620  4.355994  3.793556  0.224908\n",
       "300                     4.336677  4.524341  4.104159  0.207311\n",
       "400                     4.526428  4.832583  4.236853  0.276685\n",
       "500                     4.776828  5.174206  4.431874  0.353029\n",
       "600                     4.981517  5.533187  4.572223  0.446839\n",
       "700                     5.079321  5.690763  4.668314  0.482136\n",
       "800                     5.214527  5.876961  4.755728  0.515497\n",
       "900                     5.328502  5.967244  4.857674  0.542761\n",
       "1000                    5.417954  6.052870  4.922895  0.565076\n",
       "1100                    5.482271  6.166809  4.935299  0.603142\n",
       "1200                    5.530989  6.228631  4.990440  0.610849\n",
       "1300                    5.568979  6.273621  4.995877  0.634239\n",
       "1400                    5.622214  6.365644  5.037643  0.656974\n",
       "1500                    5.702749  6.498899  5.143312  0.664541\n",
       "1600                    5.751283  6.590142  5.191261  0.680211\n",
       "1700                    5.807898  6.713574  5.233725  0.702568\n",
       "1800                    5.850186  6.757181  5.280823  0.698126\n",
       "1900                    5.906923  6.790075  5.333257  0.685455\n",
       "2000                    5.946962  6.843815  5.336309  0.699718"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to understand these results with a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-518bf1d8a494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_cumulative_regret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# The next two lines prevent Bokeh from opening the graph in a new window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ray-rllib/multi-armed-bandits/bokeh_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_plots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_line_with_stddev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_cumulative_regret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "from bokeh_util import plot_cumulative_regret\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "import bokeh\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_regret(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../images/rllib/LinUCB-Cumulative-Regret.png))\n",
    "\n",
    "So the _cumulative_ regret increases for the entire number of training steps for all five trials, but for larger step numbers, the amount of regret added decreases as we learn, so the graph begins to level off as the system gets better at optimizing the mean reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment we're using randomly generates data on every step, so there will always be some regret even if we train for a longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the `training_iterations` from 20 to 40. Does the characteristic behavior of cumulative regret change at higher steps?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
