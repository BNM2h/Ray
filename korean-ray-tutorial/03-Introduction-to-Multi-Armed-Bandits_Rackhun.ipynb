{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Introduction to Bandits\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Multi-Armed Bandits?\n",
    "\n",
    "A **multi-armed bandit (MAB)** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit)) is one way to implement **online machine learning**, based on probability theory. Online machine learning is the idea of updating models as data arrives, rather than training \"offline\" using batch processes and historical data. Hence, a MAB provides a means to inform sequential decision-making about a process over time in the face of uncertainty. An analogous real-world situation is the way an investment fund manager adjusts a stock portfolio in response to evolving market conditions. \n",
    "\n",
    "Multi-armed bandits are named after slot machines in casinos (a.k.a., _one-armed bandits_) where pulling an arm probabilistically produces a cash payout as a reward (or fails to do so). In a typical MAB implementation, a bandit will have N “arms”, one for each of N possible actions to take at a given time step.\n",
    "\n",
    "For simplicity, we'll often just say \"bandits\" to mean MABs.\n",
    "\n",
    "---------------------\n",
    "\n",
    "**MAB**는 확률론을 기반으로 **online machine learning**을 구현할 수 있는 방법중 하나 입니다. **online machine learning**이라는 것은 이전에 우리가 알고있던 배치단위의 이미 준비된 데이터를 이용한 학습인 **\"offline\"** 방법이 아니라, 데이터가 만들어져 주어지는 동시에 바로 모델을 업데이트 할 수 있는 방법입니다. 이러한 특징을 기반으로, **MAB**는 불확실성이 존재하는 환경에서 순차적으로 의사결정을 할 수 있도록 쓰일 수 있습니다. 예를 들어, 펀드 매니져가 수시로 변하는 불확실한 주식시장에 대응하기위해 매번 포트폴리오를 조정하는 것과 비슷하다 할 수 있습니다.\n",
    "\n",
    "'Multi-armed bandits'이라는 말은 카지노에서 막대를 잡아당기면 확률적으로 상금이 나오는 슬롯머신 (a.k.a., _one-armed bandits_)에서 비롯되었습니다. 일반적으로 **MAB**를 구현시, 하나의 **\"bandit\"** 은 N개의 **\"arms\"** 를 가지고 있는데, 이는 매 시간단위 마다 가능한 N개의 행동을 의미합니다.\n",
    "\n",
    "앞으로는, 간편하게  **MABs**를 **\"bandits\"** 로 사용하겠습니다.\n",
    "\n",
    "[참고 블로그](https://brunch.co.kr/@chris-song/62)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandits vs. General Reinforcement Learning\n",
    "\n",
    "Bandits, in their simplest form, are have two restrictions compared to general reinforcement learning:\n",
    "\n",
    "1. They do not attempt to learn the best actions to take for more than one state, which is either stationary or slowly evolving. \n",
    "2. The action taken only affects the reward received, but not the next state of the environment. Recall from our [01 Introduction to Reinforcement Learning](../01-Introduction-to-Reinforcement-Learning.ipynb) lesson that RL is based on _Markov Decision Processes_, where the next state of the system is determined in part by the action taken.\n",
    "\n",
    "\n",
    "------------------------\n",
    "\n",
    "**강화학습과 MAD의 차이점**\n",
    "\n",
    "**Bandits**는 일반적인 강화학습과 비교해서 2가지 제한점이 있습니다.\n",
    "\n",
    "1. 여러가지 **state**에 대해서 최적의 **action**을 배우려고 하지 않기에 학습이 정적이거나 느릴 수 있습니다.\n",
    "2. **Bandits**에서 **action**은 따라오는 **reward**에만 영향을 미칠뿐, **next state**와는 관련이 없습니다. 1장 내용 [01 Introduction to Reinforcement Learning](../01-Introduction-to-Reinforcement-Learning.ipynb)과 비교해보면 일반적인 강화학습에서는 **action**이 **next state**에 부분적으로 영향을 미치게 되고, 이를 기반으로 _Markov Decision Processes_ 가 진행된다는 것을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits\n",
    "\n",
    "A **contextual bandit** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit), [Ray documentation](https://docs.ray.io/en/latest/rllib-algorithms.html#contextual-bandits-contrib-bandits)) relaxes the first restriction by adding access to a state of the environment, which is called the _context_, that influences the decisions that are made. At each time step, the algorithm observes this context, which can change significantly from one step to the next, and decides to choose one action among the available alternatives, then observes the outcome/reward of that decision. The environment can change randomly between steps.\n",
    "\n",
    "Returning to the inspiration of slot machines, a machine might change the images or colors it shows on the display to indicate a new context. Internally, this might involve a change in the probability of a payout. This change could happen between each pull of the arm.\n",
    "\n",
    "So, contextual bandits are a step closer to full RL, but the second difference remains, that the state is not influenced by the action taken.\n",
    "\n",
    "[Sutton 2018](../06-RL-References.ipynb#Books) uses the term _situation_ as a generic term for the environment's state and contextual bandits are called _associative search_ bandits, because they search for the best actions, but associated to a particular state (situation) value. Hence, non-contextual bandits are called _non-associative_. \n",
    "\n",
    "-----------------------------\n",
    "\n",
    "**Contextual bandit** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit), [Ray documentation](https://docs.ray.io/en/latest/rllib-algorithms.html#contextual-bandits-contrib-bandits))은 **action**과 **state**가 서로 영향을 줄 수 있는 **_context_** 라고하는 개념을 추가하여 첫번째(?) 제한점을 극복하고자 했습니다. 매 시간단위 마다 상황에 맞게 유연하게 변화하는 **context**를 기반으로, **action**을 결정하고, 그에 대한 결과 및 **reward**를 모니터링 합니다. 이때 **environment**는 매 순간 임의로 변할 수도 있다.\n",
    "\n",
    "실제 슬롯머신은 새로운 게임 판(context)마다 보여지는 그림이나 색깔이 수시로 변한다는 점을 생각해보자. 이러한 점은 내부적으로 상금을 받을 수 있을 확률이 변할 수 있다는고 할 수 있다. 다시 말해, 매번 막대를 당길때 마다 상금을 받을 수 있는 확률이 변할 수 있는 시간에 따른 불확실성이 존재한다 할 수 있다.\n",
    "\n",
    "그럼에도 불구하고, **contextual bandit**은 일반적인 강화학습과 비교했을때 여전히 존재하는 차이점은 **state**가 **action**의 영향을 받지는 않는다는 점이다.  \n",
    "\n",
    "[Sutton 2018](../06-RL-References.ipynb#Books)에서는 **environment**의 **state**를 설명하기위해 **_situation_** 이라는 용어를 사용하며, **contextual bandits**을 **associated search bandits**라고 쓰고 있는데, 이는 **contextual bandits**이 관련된 특정 **state**의 **value**에 따른 최적의 **action**을 찾기 때문이라 할 수 있다. 따라서, 여기서 **non-contextual bandits**는 **_non-associative_** 라 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward vs. Regret\n",
    "\n",
    "While it's common in reinforcement learning to maximize **rewards**, we can also model the opposite of reward, called **regret**. Essentially, regret is the difference between the reward actually achieved and the best possible reward, if the algorithm could choose the best decision at every step.\n",
    "\n",
    "The point of using a bandit is to try to _maximize average reward_. If the algorithm had perfect visibility into the dynamics of the environment, it would always choose the optimal arm to maximize the cumulative reward! However, that's not an interesting real-world problem to solve. Instead, we focus on the case where the algorithm only has limited visibility, so it cannot predict rewards. This means that the **exploitation vs. exploration tradeoff** is an essential aspect of bandits that is shared with general reinforcement learning systems. Pulling one arm repeatedly may provide an excellent reward, but other arms may provide even better rewards, especially as the context evolves.\n",
    "\n",
    "Even while the reward can't be predicted, some the exploration-exploitation strategies can place theoretical upper bounds on the regret. The use of machine learning to leverage the context and then minimize the regret in turn can provide good solutions. We'll explore this idea in the lessons.\n",
    "\n",
    "---------------------\n",
    "\n",
    "일반적으로 강화학습은 **rewards**을 최대화 하도록 학습을 하는데, 반대의 개념으로 **regret**을 사용할 수도 있다. **Regret**은 특정상황에서 최대로 받을 수 있는 **reward**와, **action**으로부터 받은 **reward**간의 차이라고 할 수 있습니다.\n",
    "\n",
    "**Bandit**은 평균적인 **reward**의 최대화(_maximize average reward_)에 그 목적을 두고 있습니다. 만약 알고리즘이 **environment**에 대해서 정확하게 파악하고 있다면, 항상 최대의 **reward**를 받도록 최적의 선택(**arm**)을 할 것 입니다! 하지만, 우리가 풀어야할 현실적인 문제들은 그렇지 않습니다. 현실 세계에서는 불확실하고 제한적인 조건들로 인해 정확한 **reward**를 예측하기 힘들기에 우리는 그런 부분들에 집중해야 합니다. 이러한 관점에서 일반적인 강화학습과 마찬가지로 **bandits**에서도 **exploitation vs. exploration tradeoff** 이 중요하다 할 수 있습니다. 하나의 막대(**arm**)만 반복해서 선택한다면, 물론 좋은 **reward** 받을지도 있겠지만, 다른 **arms**들을 선택했을때 더 좋은 **reward**를 받을 수도 있습니다.      \n",
    "\n",
    "**Reward**를 정확하게 예측 할 수는 없지만, **exploitation vs. exploration tradeoff**를 통해 이론적으로 **regret**의 상한점을 구할 수는 있습니다. 머신러닝을 통해 **context**를 활용하고, **regret**을 최소화 한다면 좋은 솔루션을 얻을 수 있습니다. 이번 장에서는 이러한 아이디어에 대해 좀 더 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating these concepts to the investment fund example, market conditions define the context. Imagine having one arm of the contextual bandit for each of the stocks that could be selected as an investment. For example, when a particular arm gets “pulled” $10,000 gets invested into that stock for that week. Subsequent performance of the selected stocks defines the _reward_. The difference between this reward and the subsequent performance of the best-performing stock in the market defines the _regret_.\n",
    "\n",
    "Of course, this example grossly oversimplified how investment fund management actually works, but it provides a good idea of how contextual bandits can be used for online learning problems. Indeed, sophisticated bandit models are used in finance.\n",
    "\n",
    "---------------\n",
    "\n",
    "펀드 투자를 예시로 들면, 시장 조건들을 **context**라 정의할 수 있습니다. 하나의 주식을 선택해서 투자를 한다는 것은 하나의 **contextual bandit**의 **arm**을 선택한다는 것에 비유 할 수 있습니다. 예를들어, 하나의 **arm**을 당기게 되면, 해당하는 주식에 한 주 동안(단위시간) $10,000를 투자했다고 할 수 있습니다. 투자에 대한 수익 결과는 **_regret_** 라 할 수 있습니다. 그리고 **_regret_** 은 해당 기간동안 가장 많이 오른 주식에 투자했을 때의 수익과 나의 투자에 대한 수익간의 차이라고 할 수 있습니다. \n",
    "\n",
    "물론, 지금의 예는 실제 펀트 투자를 굉장히 단순화한 것에 불구하지만, **contextual bandits**이 실제로 **online learning**문제에서 어떻게 사용될 수 있는지 보여주고 있습니다. 실제 경제분야에서는 더 복잡하고 정교한 **bandit**모델들이 사용되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology Summary\n",
    "\n",
    "Let's summarize terms we've defined already and review RL terms we already know:\n",
    "\n",
    "* **State:** The state of the environment is its “context”.\n",
    "* **Action:** The decision taken, i.e., which arm is pulled.\n",
    "* **Agent:** Same as before; the agent decides what action to take, based on a policy for the environment.\n",
    "* **Reward:** The observed reward received by taking the last action and the cumulative rewards for all actions.\n",
    "* **Regret:** The difference between the reward that could be achieved by always selecting the optimal arm and the reward actually achieved by the algorithm.\n",
    "* **Exploration/exploitation tradeoff:** Balancing when to exploit the best rewarding arm known vs. trying different choices to learn more about the possibility of even better rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Bandits have been applied to a wide variety of applications. Another practical use of bandits is testing alternative designs (like websites), often with greater flexibility and less cost than conventional A/B testing. Many example uses are described in [this paper](https://arxiv.org/abs/1904.10040):\n",
    "\n",
    "1. Determining the correct doses for individuals of widely-used medications, where the \"context\" is the individual's health data.\n",
    "2. Modeling brain and behaviorial disorders, accounting for sequential patterns.\n",
    "3. Portfolio management, balancing risk vs. reward and accounting for correlated behaviors between \"instruments\" (e.g., particular stocks).\n",
    "4. Dynamic pricing of goods and services based on demand and feature sets.\n",
    "5. Recommendation systems when the user and item data sets are too large for traditional collaborative filtering approaches and also to better account for users' evolving preferences.\n",
    "6. Information retrieval that accounts for iterative retrieval and likely exploration paths using the results.\n",
    "7. Dialog systems, where the sequential, online aspect of MABs is a better fit for dialogs than offline supervised learning, and also enables the dialog systems to continue learning as they interact with users.\n",
    "8. Anomaly detection, where the exploitation, exploration tradeoff informs how suspected anomalies are investigated, including which ones to pursue or ignore, based on relative likely significance.\n",
    "9. Frequency hopping, \"spreading\", and power control in telecommunications applications modeled and optimized to improve the end user experience."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
