{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - A Simple Bandit Example\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with three arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. \n",
    "\n",
    "세 개의 팔을 가진 아주 간단한 contextual bandit 사례를 살펴보자. RLlib와 Ray의 하이퍼 파라미터 튜닝 라이브러리인 [Tune](http://tune.io)를 사용하여 테스트를 실행하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random, time\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bandit as a subclass of an OpenAI Gym environment. We set the action space to have three discrete variables, one action for each arm, and an observation space (the context) in the range -1.0 to 1.0, inclusive. (See the [configuring environments](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments) documentation for more details about creating custom environments.)\n",
    "\n",
    "There are two contexts defined. Note that we'll randomly pick one of them to use when `reset` is called, but it stays fixed (static) throughout the episode (the set of steps between calls to `reset`).\n",
    "\n",
    "우리는 bandit을 OpenAI Gym 환경의 하위 클래스로 정의한다. action space는 3개의 이산형 변수로,각 암에 대해 1개의 action을 가진다. observation(context)는 -1.0 ~ 1.0 범위를 가진다. \n",
    "([configuring environments](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments) 사용자 지정 환경 생성에 대한 자세한 내용은 환경 구성 문서를 참조하십시오)\n",
    "\n",
    "두 가지의 context가 정의되어 있다. 리셋이 호출될 때 사용할 수 있도록 임의로 하나를 선택하지만, 에피소드 내내 고정(정적) 상태를 유지한다는 점에 유의하여야한다.(리셋할 호출 사이의 단계 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {        # 2 contexts: -1 and 1\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "        #return observation which is static\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the definition of `self.rewards_for_context`. For context `-1.`, choosing the **third** arm (index 2 in the array) maximizes the reward, yielding `10.0` for each pull. Similarly, for context `1.`, choosing the **first** arm (index 0 in the array) maximizes the reward. It is never advantageous to choose the second arm.\n",
    "\n",
    "We'll see if our training results agree ;)\n",
    "\n",
    "self.rewards_for_context의 정의를 보자. context `-1.` 의 경우, **3** 암(배열에서 색인 2)을 선택하면 각 당김에 대해 10.0이 주어지며 보상이 최대화된다. 마찬가지로 context  `1.`의 경우 **1** 암(배열에서 색인 0)을 선택하면 보상이 최대화된다. 두 번째 팔을 선택하는 것은 결코 유리하지 않다.\n",
    "\n",
    "훈련 결과가 일치하는지 확인할 것이다 ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try repeating the next two code cells enough times to see the `current_context` set to `1.0` and `-1.0`, which is initialized randomly in `reset()`.\n",
    "\n",
    "`reset()`에서 무작위로 초기화되는 `current_context`가 1.0과-1.0으로 설정되는 것을 볼 수 있을 만큼 다음 두 개의 코드 셀을 반복해 보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [-1.  1.], bandit = SimpleContextualBandit(action_space=Discrete(3), observation_space=Box(2,), current_context=1.0, rewards per context={-1.0: [-10, 0, 10], 1.0: [10, 0, -10]})'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleContextualBandit()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bandit.current_context` and the observation of the current environment will remain fixed through the episode.\n",
    "\n",
    "`bandit.current_context`와 현재 환경에 대한 observation은 에피소드를 통해 고정된 상태를 유지할 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_context = 1.0\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], action = 0, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], action = 0, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n"
     ]
    }
   ],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `current_context`. If it's `1.0`, does the `0` (first) action yield the highest reward and lowest regret? If it's `-1.0`, does the `2` (third) action yield the highest reward and lowest regret? The `1` (second) action always returns `0` reward, so it's never optimal. \n",
    "\n",
    "`current_context`를 보자. `1.0` 이라면 `0` (첫 번째) 행동은 가장 높은 reward과 가장 낮은 regret를 주는 것일까? `-1.0`이라면 `2(3번째)` 행동이 가장 높은 reward과 가장 낮은 regret을 주는가? \n",
    "`1(두 번째)` 행동은 항상 0의 보상을 돌려주기 때문에 결코 최적이 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LinUCB\n",
    "\n",
    "For this simple example, we can easily determine the best actions to take. Let's see how well our system does. We'll train with [LinUCB](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb), a linear version of _Upper Confidence Bound_, for the exploration-exploitation strategy. _LinUCB_ assumes a linear dependency between the expected reward of an action and its context. Recall that a linear function is of the form $z = ax + by + c$, for example, where $x$, $y$, and $z$ are variables and $a$, $b$, and $c$ are constants. _LinUCB_ models the representation space using a set of linear predictors. Hence, the $Q_t(a)$ _value_ function discussed for UCB in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb) is assumed to be a linear function here.\n",
    "\n",
    "\n",
    "이 간단한 예를 위해, 우리는 가장 좋은 조치를 쉽게 결정할 수 있다. 일단 우리 시스템이 얼마나 잘 작동하는지 봅시다. 우리는 [LinUCB](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb), 에서 exploration-exploitstion 전략을 위해 선형 버전의 _Upper Confidence Bound_와 함께 훈련할 것이다. _LinUCB_는 어떤 행동의 예상 보상과 그 상황 사이의 선형 종속성을 가정한다.\n",
    "\n",
    "선형 함수는 $z = ax + by + c$ 형식이며, $x$, $y$, $z$는 변수, $a$, $b$, $c$는 상수라는 점을 기억하자. _LinUCB_는 일련의 선형 예측 변수를 사용하여 표현 공간을 모형화한다. 따라서 [previous lesson](02-탐색-vs-탐색-전략.ipynb)에서 UCB에 대해 논의한 $Q_t(a)$ _value_ 함수는 여기서 선형 함수로 가정한다.\n",
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "Look again at how we defined `rewards_for_context`. Is it linear as expected for _LinUCB_?\n",
    "\n",
    "우리가 reward_for_context를 어떻게 정의했는지 다시 한 번 살펴보십시오. LinUCB의 예상대로 선형인가?\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Yes, for each arm, the reward is linear in the context. For example, the first arm has a reward of `-10` for context `-1.0` and `10` for context `1.0`. Crucially, the _same_ linear function that works for the first arm will work for the other two arms if you multiplied the constants in the linear function by `0` and `-1`, respectively. Hence, we expect _LinUCB_ to work well for this example.\n",
    "\n",
    "그렇다! 각 팔마다 reward는 context에 대하여 선형이다. 예를 들어, 첫 번째 팔은 컨텍스트 `-1.0`에 대해 `-10`, 컨텍스트 `1.0`에 대해 `10`의 보상을 받는다. 결정적으로 첫 번째 암에 작용하는 동일한 선형 함수는 선형 함수의 상수를 각각 `0`과 `-1`로 곱하면 다른 두 암에 대해 작용한다. 따라서 우리는 이 예에서 LinUCB가 잘 작동하기를 기대한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Tune to train the policy for this bandit. But first, we want to start Ray on your laptop or connect to the running Ray cluster if you are working on the Anyscale platform.\n",
    "\n",
    "이제 Tune을 사용하여 이 bandit의 정책을 훈련해보자. \n",
    "하지만 먼저, Anyscale 플랫폼에서 작업하는 경우 랩톱에서 Ray를 시작하거나 실행 중인 Ray 클러스터에 연결하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-29 15:10:09,704\tINFO resource_spec.py:231 -- Starting Ray with 3.37 GiB memory available for workers and up to 1.71 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-29 15:10:10,406\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.60',\n",
       " 'raylet_ip_address': '192.168.2.60',\n",
       " 'redis_address': '192.168.2.60:29414',\n",
       " 'object_store_address': '/tmp/ray/session_2020-08-29_15-10-09_702756_6627/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-08-29_15-10-09_702756_6627/sockets/raylet',\n",
       " 'webui_url': 'localhost:8266',\n",
       " 'session_dir': '/tmp/ray/session_2020-08-29_15-10-09_702756_6627'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `ray.tune.run` below would handle Ray initialization for us, if Ray isn't already running. If you want to prevent this and have Tune exit with an error when Ray isn't already initialized, then pass `ray_auto_init=False`.\n",
    "\n",
    "Ray가 돌아가지 않는 상황에서 `Ray.tun.run`을 실행하면 Ray 초기화를 처리할 수 있다. Ray가 아직 초기화되지 않은 상태에서 이를 방지하고 Tune exit를 에러 없이 사용하고 싶다면 `ray_auto_init=False`를 전달하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-29 15:10:14,475\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.1/7.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/4 CPUs, 0/0 GPUs, 0.0/3.37 GiB heap, 0.0/1.17 GiB objects<br>Result logdir: /home/jhmbabo/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                       </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_4e4b4_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6699)\u001b[0m 2020-08-29 15:10:16,582\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=6699)\u001b[0m 2020-08-29 15:10:17,109\tINFO trainer.py:632 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=6699)\u001b[0m 2020-08-29 15:10:17,145\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_LinUCB_SimpleContextualBandit_4e4b4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-08-29_15-10-17\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: fd90e5ceb3874e6f94eba2ef888dd7ae\n",
      "  experiment_tag: '0'\n",
      "  hostname: k-14z970-gr30k\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cumulative_regret: 10\n",
      "        update_latency: 0.00042057037353515625\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.60\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.8\n",
      "    ram_util_percent: 42.1\n",
      "  pid: 6699\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04692596964316795\n",
      "    mean_inference_ms: 0.8593526217016847\n",
      "    mean_processing_ms: 0.5114409002927269\n",
      "  time_since_restore: 0.20624709129333496\n",
      "  time_this_iter_s: 0.20624709129333496\n",
      "  time_total_s: 0.20624709129333496\n",
      "  timers:\n",
      "    learn_throughput: 2305.956\n",
      "    learn_time_ms: 0.434\n",
      "    sample_throughput: 701.572\n",
      "    sample_time_ms: 1.425\n",
      "  timestamp: 1598681417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: 4e4b4_00000\n",
      "  \n",
      "Result for contrib_LinUCB_SimpleContextualBandit_4e4b4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-08-29_15-10-17\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 200\n",
      "  experiment_id: fd90e5ceb3874e6f94eba2ef888dd7ae\n",
      "  experiment_tag: '0'\n",
      "  hostname: k-14z970-gr30k\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cumulative_regret: 10\n",
      "        update_latency: 0.0006177425384521484\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.2.60\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 6699\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.051499599248022596\n",
      "    mean_inference_ms: 0.8830846245609113\n",
      "    mean_processing_ms: 0.5431732728113584\n",
      "  time_since_restore: 0.436875581741333\n",
      "  time_this_iter_s: 0.23062849044799805\n",
      "  time_total_s: 0.436875581741333\n",
      "  timers:\n",
      "    learn_throughput: 1208.907\n",
      "    learn_time_ms: 0.827\n",
      "    sample_throughput: 417.606\n",
      "    sample_time_ms: 2.395\n",
      "  timestamp: 1598681417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: 4e4b4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.2/7.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.37 GiB heap, 0.0/1.17 GiB objects<br>Result logdir: /home/jhmbabo/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                       </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_4e4b4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.436876</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">      10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=2, #ray_auto_init=False   # Change to 0 or 1 to reduce the output.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A lot of output is printed with `verbose` set to `2`. Use `0` for no output and `1` for short summaries.)\n",
    "\n",
    "(많은 출력물은 `verbose`가 `2`로 설정되어 인쇄된다. 출력 없음에는 `0`을 사용하고, 짧은 요약에는 `1`을 사용한다.)\n",
    "\n",
    "How long did it take?\n",
    "\n",
    "얼마나 걸렸는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2.18 seconds,    0.04 minutes\n"
     ]
    }
   ],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the final data as a dataframe:\n",
    "\n",
    "최종 데이터의 일부를 데이터 프레임으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>timers/learn_time_ms</th>\n",
       "      <th>timers/learn_throughput</th>\n",
       "      <th>info/num_steps_sampled</th>\n",
       "      <th>info/num_steps_trained</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/default_policy/cumulative_regret</th>\n",
       "      <th>info/learner/default_policy/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827</td>\n",
       "      <td>1208.907</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>&lt;class '__main__.SimpleContextualBandit'&gt;</td>\n",
       "      <td>/home/jhmbabo/ray_results/contrib/LinUCB/contr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                10.0                 10.0   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0               1.0                 100                    0              200   \n",
       "\n",
       "   done  episodes_total  training_iteration  ... timers/learn_time_ms  \\\n",
       "0  True             200                   2  ...                0.827   \n",
       "\n",
       "  timers/learn_throughput  info/num_steps_sampled  info/num_steps_trained  \\\n",
       "0                1208.907                     200                     200   \n",
       "\n",
       "   perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                    NaN                    NaN   \n",
       "\n",
       "  info/learner/default_policy/cumulative_regret  \\\n",
       "0                                            10   \n",
       "\n",
       "  info/learner/default_policy/update_latency  \\\n",
       "0                                   0.000618   \n",
       "\n",
       "                                  config/env  \\\n",
       "0  <class '__main__.SimpleContextualBandit'>   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/jhmbabo/ray_results/contrib/LinUCB/contr...  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to inspect the progression of training is to use TensorBoard.\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _TensorBoard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```\n",
    "\n",
    "훈련 진행 상황을 점검하는 가장 쉬운 방법은 텐서보드를 이용하는 것이다.\n",
    "\n",
    "1. Anyscale Platform에서 실행 중인 경우 _TensorBoard_ 링크를 클릭하십시오.\n",
    "2. 노트북에서 이 노트북을 실행 중인 경우 _Edit_ 메뉴의 '+'를 사용하여 터미널 창을 열고 다음 명령을 실행한 후 표시된 URL을 여십시오.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have many data sets plotted from previous tutorial lessons. In the _Runs_ on the left, look for one named something like this:\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "If you have several of them, you want the one with the latest timestamp. To select just that one, click _toggler all runs_ below the list of runs, then select the one you want. You should see something like [this image](../../images/rllib/TensorBoard1.png).\n",
    "\n",
    "The graph for the metric we were optimizing, the mean reward, is shown with a rectangle surrounding it. It improved steadily during the training runs. For this simple example, the reward mean is easily found in 200 steps.\n",
    "\n",
    "\n",
    "이전 튜토리얼 수업에서 플로팅된 데이터 세트가 많을 수 있다. 왼쪽의 _Runs_에서 다음과 같은 이름을 가진 것을 찾으십시오.\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "여러 개의 타임스탬프가 있는 경우 최신 타임스탬프가 있는 타임스탬프를 사용하십시오. 이 항목만 선택하려면 실행 목록 아래에 있는 _모든 실행 전환_을 클릭한 다음 원하는 실행을 선택하십시오.[this image](../../images/rllib/TensorBoard1.png).\n",
    "\n",
    "우리가 최적화하고 있던 메트릭스 그래프, 평균 보상은 그것을 둘러싼 직사각형과 함께 표시된다. 훈련 기간 동안 꾸준히 향상되었다. 이 간단한 예로 보상 평균은 200단계에서 쉽게 찾을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the the `step` method to randomly change the `current_context` on each invocation:\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    result = super().step(action)\n",
    "    self.current_context = random.choice([-1.,1.])\n",
    "    return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "            {\n",
    "                \"regret\": 10 - reward\n",
    "            })\n",
    "```\n",
    "\n",
    "Repeat the training and analysis. Does the training behavior change in any appreciable way? Why or why not?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (Optional)\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue exploring usage of _LinUCB_ in the next lesson, [04 Linear Upper Confidence Bound](04-Linear-Upper-Confidence-Bound.ipynb) and _LinTS_ in the following lesson, [05 Thompson Sampling](05-Linear-Thompson-Sampling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
