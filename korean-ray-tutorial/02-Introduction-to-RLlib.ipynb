{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to RLlib\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "In the [previous lesson](01-Introduction-to-Reinforcement-Learning.ipynb), we learned the basic concepts of reinforcement learning, with a \"taste\" of [RLlib](https://rllib.io) and [OpenAI Gym](https://gym.openai.com). This lesson takes a step back to provide more information about RLlib and the features it provides. The subsequent lessons will continue our exploration of RL concepts and RLlib tools.\n",
    "\n",
    "For more detailed information about RLlib and its open source community, see the following:\n",
    "\n",
    "* [rllib.io](http://rllib.io) (the documentation)\n",
    "* [GitHub repo](https://github.com/ray-project/ray/tree/master/rllib#rllib-scalable-reinforcement-learning)\n",
    "\n",
    "이 수업은 RLlib과 그것이 제공하는 기능에 대한 더 많은 정보를 제공하기 위해 잠시 한 발짝 뒤로 간다. 이후 수업은 RL 개념과 RLlib 도구에 대해 계속 공부할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLlib is structured conceptually like this:\n",
    "\n",
    "![RLlib Stack](../images/rllib/RLlib-Stack-smaller.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _(1) Application Support_ boxes are components used for particular RL algorithms. \n",
    "\n",
    "The _(2) Abstractions for RL_ provide building blocks used by the many algorithms that are implemented in RLlib (listed below). They also provide hooks for implementing your own algorithms. \n",
    "\n",
    "RLlib leverages Ray for efficient, cluster-wide, _(3) Distributed Execution_.\n",
    "\n",
    "_(1) Application Support_: 특정 RL 알고리즘에 사용되는 구성 요소다. \n",
    "\n",
    "_(2) Abstractions for RL_: RLlib에서 구현되는 많은 알고리즘에서 사용되는 구성 블록을 제공한다(아래 리스트 참고). 또한 커스텀 알고리즘을 구현하기 위한 후크를 제공한다.\n",
    "\n",
    "_(3) Distributed Execution_: Ray의 efficient, cluster-wide한 실행을 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start up Ray as in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib in 60 Seconds (plus some...)\n",
    "\n",
    "Here is a fast introduction to using RLlib from a command line, adapted from the [documentation](https://docs.ray.io/en/latest/rllib.html#rllib-in-60-seconds).\n",
    "\n",
    "First you would install [PyTorch](http://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/), whichever you prefer.  Then install RLlib. **All of these items are already installed in this tutorial environment.**\n",
    "\n",
    "```shell\n",
    "pip install ray[rllib]  # or consider using: ray[debug]\n",
    "```\n",
    "\n",
    "Then **train** `CartPole` using _PPO_ with the `rllib` CLI. We connect to the running Ray cluster, we'll stop at 20 iterations, and we'll save checkpoints every 10 iterations and at the end:\n",
    "\n",
    "```shell\n",
    "rllib train --run PPO --env CartPole-v1 --stop='{\"training_iteration\": 20}' --ray-address auto --checkpoint-freq 10 --checkpoint-at-end\n",
    "```\n",
    "\n",
    "The `rllib` CLI has a `--help` flag that prints details about the supported options:\n",
    "\n",
    "```shell\n",
    "rllib --help          # general help\n",
    "rllib train --help    # specific help on the training options\n",
    "rllib rollout --help  # specific help on the rollout options\n",
    "```\n",
    "\n",
    "_Rollout_ means running an episode with the trained model, which you specify by passing a checkpoint directory to the command. During rollout a continuous loop of taking an action and observing the new state and reward continues until some final state or number of iterations is reached.\n",
    "\n",
    "You can execute the same training logic using the following Python code, which leverages [Ray Tune](http://tune.io), specifically the [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run) method:\n",
    "\n",
    "```python\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, \n",
    "    config={\"env\": \"CartPole-v1\"},\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=2            # 2 for INFO; change to 1 or 0 to reduce the output.\n",
    "    )  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the `rllib` CLI just shown. The following cell will take between one and two minutes to run.\n",
    "\n",
    "You could also run this command in a separate terminal window.\n",
    "\n",
    "> **Tip:** The output will be long. When this happens for a cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jhmbabo/anaconda3/envs/tf1/bin/rllib\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/ray/rllib/scripts.py\", line 34, in cli\n",
      "    train.run(options, train_parser)\n",
      "  File \"/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/ray/rllib/train.py\", line 220, in run\n",
      "    local_mode=args.local_mode)\n",
      "  File \"/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/ray/worker.py\", line 654, in init\n",
      "    address, redis_address)\n",
      "  File \"/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/ray/services.py\", line 276, in validate_redis_address\n",
      "    address = find_redis_address_or_die()\n",
      "  File \"/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/ray/services.py\", line 168, in find_redis_address_or_die\n",
      "    \"Could not find any running Ray instance. \"\n",
      "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting `address`.\n"
     ]
    }
   ],
   "source": [
    "!rllib train --run PPO --env CartPole-v1 --stop='{\"training_iteration\": 40}' --ray-address auto --checkpoint-freq 10 --checkpoint-at-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /home/jhmbabo/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "usage: rllib train [-h] [--run RUN] [--stop STOP] [--config CONFIG]\n",
      "                   [--resources-per-trial RESOURCES_PER_TRIAL]\n",
      "                   [--num-samples NUM_SAMPLES]\n",
      "                   [--checkpoint-freq CHECKPOINT_FREQ] [--checkpoint-at-end]\n",
      "                   [--sync-on-checkpoint]\n",
      "                   [--keep-checkpoints-num KEEP_CHECKPOINTS_NUM]\n",
      "                   [--checkpoint-score-attr CHECKPOINT_SCORE_ATTR]\n",
      "                   [--export-formats EXPORT_FORMATS]\n",
      "                   [--max-failures MAX_FAILURES] [--scheduler SCHEDULER]\n",
      "                   [--scheduler-config SCHEDULER_CONFIG] [--restore RESTORE]\n",
      "                   [--ray-address RAY_ADDRESS] [--no-ray-ui] [--local-mode]\n",
      "                   [--ray-num-cpus RAY_NUM_CPUS] [--ray-num-gpus RAY_NUM_GPUS]\n",
      "                   [--ray-num-nodes RAY_NUM_NODES]\n",
      "                   [--ray-redis-max-memory RAY_REDIS_MAX_MEMORY]\n",
      "                   [--ray-memory RAY_MEMORY]\n",
      "                   [--ray-object-store-memory RAY_OBJECT_STORE_MEMORY]\n",
      "                   [--experiment-name EXPERIMENT_NAME] [--local-dir LOCAL_DIR]\n",
      "                   [--upload-dir UPLOAD_DIR] [-v] [-vv] [--resume] [--torch]\n",
      "                   [--eager] [--trace] [--env ENV] [--queue-trials]\n",
      "                   [-f CONFIG_FILE]\n",
      "\n",
      "Train a reinforcement learning agent.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --run RUN             The algorithm or model to train. This may refer to the\n",
      "                        name of a built-on algorithm (e.g. RLLib's DQN or\n",
      "                        PPO), or a user-defined trainable function or class\n",
      "                        registered in the tune registry.\n",
      "  --stop STOP           The stopping criteria, specified in JSON. The keys may\n",
      "                        be any field returned by 'train()' e.g.\n",
      "                        '{\"time_total_s\": 600, \"training_iteration\": 100000}'\n",
      "                        to stop after 600 seconds or 100k iterations,\n",
      "                        whichever is reached first.\n",
      "  --config CONFIG       Algorithm-specific configuration (e.g. env,\n",
      "                        hyperparams), specified in JSON.\n",
      "  --resources-per-trial RESOURCES_PER_TRIAL\n",
      "                        Override the machine resources to allocate per trial,\n",
      "                        e.g. '{\"cpu\": 64, \"gpu\": 8}'. Note that GPUs will not\n",
      "                        be assigned unless you specify them here. For RLlib,\n",
      "                        you probably want to leave this alone and use RLlib\n",
      "                        configs to control parallelism.\n",
      "  --num-samples NUM_SAMPLES\n",
      "                        Number of times to repeat each trial.\n",
      "  --checkpoint-freq CHECKPOINT_FREQ\n",
      "                        How many training iterations between checkpoints. A\n",
      "                        value of 0 (default) disables checkpointing.\n",
      "  --checkpoint-at-end   Whether to checkpoint at the end of the experiment.\n",
      "                        Default is False.\n",
      "  --sync-on-checkpoint  Enable sync-down of trial checkpoint to guarantee\n",
      "                        recoverability. If unset, checkpoint syncing from\n",
      "                        worker to driver is asynchronous, so unset this only\n",
      "                        if synchronous checkpointing is too slow and trial\n",
      "                        restoration failures can be tolerated.\n",
      "  --keep-checkpoints-num KEEP_CHECKPOINTS_NUM\n",
      "                        Number of best checkpoints to keep. Others get\n",
      "                        deleted. Default (None) keeps all checkpoints.\n",
      "  --checkpoint-score-attr CHECKPOINT_SCORE_ATTR\n",
      "                        Specifies by which attribute to rank the best\n",
      "                        checkpoint. Default is increasing order. If attribute\n",
      "                        starts with min- it will rank attribute in decreasing\n",
      "                        order. Example: min-validation_loss\n",
      "  --export-formats EXPORT_FORMATS\n",
      "                        List of formats that exported at the end of the\n",
      "                        experiment. Default is None. For RLlib, 'checkpoint'\n",
      "                        and 'model' are supported for TensorFlow policy\n",
      "                        graphs.\n",
      "  --max-failures MAX_FAILURES\n",
      "                        Try to recover a trial from its last checkpoint at\n",
      "                        least this many times. Only applies if checkpointing\n",
      "                        is enabled.\n",
      "  --scheduler SCHEDULER\n",
      "                        FIFO (default), MedianStopping, AsyncHyperBand,\n",
      "                        HyperBand, or HyperOpt.\n",
      "  --scheduler-config SCHEDULER_CONFIG\n",
      "                        Config options to pass to the scheduler.\n",
      "  --restore RESTORE     If specified, restore from this checkpoint.\n",
      "  --ray-address RAY_ADDRESS\n",
      "                        Connect to an existing Ray cluster at this address\n",
      "                        instead of starting a new one.\n",
      "  --no-ray-ui           Whether to disable the Ray web ui.\n",
      "  --local-mode          Whether to run ray with `local_mode=True`. Only if\n",
      "                        --ray-num-nodes is not used.\n",
      "  --ray-num-cpus RAY_NUM_CPUS\n",
      "                        --num-cpus to use if starting a new cluster.\n",
      "  --ray-num-gpus RAY_NUM_GPUS\n",
      "                        --num-gpus to use if starting a new cluster.\n",
      "  --ray-num-nodes RAY_NUM_NODES\n",
      "                        Emulate multiple cluster nodes for debugging.\n",
      "  --ray-redis-max-memory RAY_REDIS_MAX_MEMORY\n",
      "                        --redis-max-memory to use if starting a new cluster.\n",
      "  --ray-memory RAY_MEMORY\n",
      "                        --memory to use if starting a new cluster.\n",
      "  --ray-object-store-memory RAY_OBJECT_STORE_MEMORY\n",
      "                        --object-store-memory to use if starting a new\n",
      "                        cluster.\n",
      "  --experiment-name EXPERIMENT_NAME\n",
      "                        Name of the subdirectory under `local_dir` to put\n",
      "                        results in.\n",
      "  --local-dir LOCAL_DIR\n",
      "                        Local dir to save training results to. Defaults to\n",
      "                        '/home/jhmbabo/ray_results'.\n",
      "  --upload-dir UPLOAD_DIR\n",
      "                        Optional URI to sync training results to (e.g.\n",
      "                        s3://bucket).\n",
      "  -v                    Whether to use INFO level logging.\n",
      "  -vv                   Whether to use DEBUG level logging.\n",
      "  --resume              Whether to attempt to resume previous Tune\n",
      "                        experiments.\n",
      "  --torch               Whether to use PyTorch (instead of tf) as the DL\n",
      "                        framework.\n",
      "  --eager               Whether to attempt to enable TF eager execution.\n",
      "  --trace               Whether to attempt to enable tracing for eager mode.\n",
      "  --env ENV             The gym environment to use.\n",
      "  --queue-trials        Whether to queue trials when the cluster does not\n",
      "                        currently have enough resources to launch one. This\n",
      "                        should be set to True when running on an autoscaling\n",
      "                        cluster to enable automatic scale-up.\n",
      "  -f CONFIG_FILE, --config-file CONFIG_FILE\n",
      "                        If specified, use config options from this file. Note\n",
      "                        that this overrides any trial-specific options set via\n",
      "                        flags above.\n",
      "\n",
      "Training example via RLlib CLI:\n",
      "    rllib train --run DQN --env CartPole-v0\n",
      "\n",
      "Grid search example via RLlib CLI:\n",
      "    rllib train -f tuned_examples/cartpole-grid-search-example.yaml\n",
      "\n",
      "Grid search example via executable:\n",
      "    ./train.py -f tuned_examples/cartpole-grid-search-example.yaml\n",
      "\n",
      "Note that -f overrides all other trial-specific command-line options.\n"
     ]
    }
   ],
   "source": [
    "!rllib train --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view the training results using [TensorBoard](https://www.tensorflow.org/tensorboard). The results during training were written to a directory under `$HOME/ray_results`\n",
    "\n",
    "If you are viewing this lesson on the Anyscale hosted platform, use the provided link to open TensorBoard.\n",
    "\n",
    "If you are viewing this lesson on a laptop, open a terminal and run the following command, then open the URL shown in the output. (You can open a terminal using the `+` in the upper left-hand corner of Jupyter Lab.)\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=~/ray_results\n",
    "```\n",
    "\n",
    "Here is a [TensorBoard screenshot](../images/rllib/TensorBoard-CartPole-PPO.png).\n",
    "\n",
    "The directory `$HOME/ray_results` will contain the results for all the RL training we'll do in this tutorial. You may wish to clean out old results periodically. For the run we just did, look for the results in `$HOME/ray_results/default/PPO-CartPole-V1_0_YYYY-MM-DD_HH-MM-SS*`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollout\n",
    "\n",
    "> **WARNING:** The `rllib rollout` command discussed next won't work in a cloud environment, because it attempts to pop up a window. If you are taking a live class, the instructor will demonstrate what you would see. You can also watch the next video of a single _episode_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,../images/rllib/Cart-Pole-Example-Video.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_sample_video='../images/rllib/Cart-Pole-Example-Video.mp4'\n",
    "Video(cart_pole_sample_video,embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(IPython.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a laptop, you can use `rllib rollout <checkpoint> --run PPO` to run episodes from a `<checkpoint>`, which in this case will be a directory with a name like this:\n",
    "\n",
    "```\n",
    "$HOME/ray_results/default/PPO_CartPole-v1_0_YYYY-MM-DD_HH-MM-SS.../checkpoint_20/checkpoint-20/\n",
    "```\n",
    "\n",
    "The following shell command will find the correct directory for you and run the `rllib rollout <checkpoint> --run PPO` command. (It will print the actual command, with the correct checkpoint directory.) If it finds more than once checkpoint directory, for example from a previous run, it uses the latest one.\n",
    "\n",
    "> **Note:** If you are working in a cloud environment, add the flag `--no-render` to the command. Otherwise, an error will occur because RLlib won't be able to open the window discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: rollout.sh: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!rollout.sh --episodes 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this RLlib page on training policies](https://docs.ray.io/en/master/rllib-training.html) for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on RLlib Concepts: Policies, Environments, Samples, and Trainers\n",
    "\n",
    "### Policies and Environments\n",
    "\n",
    "[Policies](https://docs.ray.io/en/latest/rllib-concepts.html#policies) in RLlib are Python classes that define how an agent acts in an environment.\n",
    "\n",
    "[Rollout workers](https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. \n",
    "\n",
    "In a [gym](https://docs.ray.io/en/latest/rllib-env.html#openai-gym) environment, there is a single agent and policy. In [vector environments](https://docs.ray.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent and hierachical environments](https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.\n",
    "\n",
    "![Environments and Policies in RLlib](../images/rllib/multi-flat.svg)\n",
    "\n",
    "The RLlib documentation on [environments](https://docs.ray.io/en/latest/rllib-env.html#rllib-environments) provides more details.\n",
    "\n",
    "Policies can be implemented using any framework ([RLlib policy.py code](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py)). However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions, respectively, that let you define a trainable policy with a functional-style API. This example is taken from the [documentation](https://docs.ray.io/en/latest/rllib.html#policies):\n",
    "\n",
    "```python\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "\n",
    "From single processes to large clusters, all data interchange in RLlib uses [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `rollout_fragment_length` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD (stochastic gradient descent).\n",
    "\n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:\n",
    "\n",
    "```python\n",
    " { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "   'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "   'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "   'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "   'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "   't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}\n",
    "```\n",
    "\n",
    "In [multi-agent mode](https://docs.ray.io/en/latest/rllib-concepts.html#policies-in-multi-agent), sample batches are collected separately for each individual policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainers\n",
    "\n",
    "At a high level, RLlib provides [trainer classes](https://docs.ray.io/en/latest/rllib-concepts.html#trainers) ([Trainer source code](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py)) that hold a policy for environment interaction. Through the trainer interface, the policy can be trained, checkpointed, or an action computed. In multi-agent training, the trainer manages the querying and optimization of multiple policies at once.\n",
    "\n",
    "![RLlib API](../images/rllib/RLlib-API.svg)\n",
    "\n",
    "the trainer classes coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging Ray [parallel iterators](https://docs.ray.io/en/latest/iter.html) (see also this lesson: [Ray Crash Course: 05 Ray Parallel Iterators](../ray-crash-course/05-Ray-Parallel-Iterators.ipynb)) to implement the desired computation pattern. The following figure shows *synchronous sampling*, the simplest of [these patterns](https://docs.ray.io/en/latest/rllib-algorithms.html):\n",
    "\n",
    "![Synchronous Sampling](../images/rllib/a2c-arch.svg)\n",
    "\n",
    "    Synchronous Sampling (e.g., A2C, PG, PPO)\n",
    "\n",
    "RLlib uses [Ray actors](https://docs.ray.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://docs.ray.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter. Check out the [scaling guide](https://docs.ray.io/en/latest/rllib-training.html#scaling-guide) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies \n",
    "\n",
    "Each policy implementation defines a `learn_on_batch()` method that improves the policy given a sample batch of input. For TensorFlow and PyTorch policies, this is implemented using a _loss function_ that takes as input sample batch tensors and outputs a scalar loss value. Here are a few example loss functions:\n",
    "\n",
    "* Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py).\n",
    "* Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "* Importance-weighted _APPO surrogate loss_ for [TensorFlow](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_tf_policy.py), [PyTorch](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_torch_policy.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Data\n",
    "\n",
    "Beyond environments defined in Python,  batch training on [offline datasets](https://docs.ray.io/en/latest/rllib-offline.html) is supported. This is an important use case for RL when it's not possible to run traditional training and rollout in a physical environment (like a chemical plant or assembly line) and a suitable simulator doesn't exist. In this approach, data for past activity is used to train a policy.\n",
    "\n",
    "This is sometimes called [imitation learning](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil).\n",
    "\n",
    "## Application Support and Customization\n",
    "\n",
    "[RLlib supports]((https://docs.ray.io/en/latest/rllib.html#application-support)) a variety of integration strategies for [external applications](https://docs.ray.io/en/latest/rllib-env.html#external-agents-and-applications).\n",
    "\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environment](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://docs.ray.io/en/latest/rllib-models.html#tensorflow-models), [action distributions](https://docs.ray.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://docs.ray.io/en/latest/rllib-concepts.html#policies>).\n",
    "\n",
    "![RLlib components](../images/rllib/RLlib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Implemented in RLlib\n",
    "\n",
    "Here is the current list of supported algorithms in RLlib. The links go to the corresponding RLlib documentation, which includes links to the original papers and other references.\n",
    "\n",
    "In this tutorial, we will mostly use [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo), [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn), and the contextual bandit algorithms, [Linear Upper Confidence Bound (LinUCB)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-upper-confidence-bound-contrib-linucb) and [Linear Thompson Sampling (LinTS)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-thompson-sampling-contrib-lints).\n",
    "\n",
    "See also the documentation's [Feature Compatibility Matrix](https://docs.ray.io/en/latest/rllib-algorithms.html#feature-compatibility-matrix), which lists the algorithms and useful properties for them. It appears at the beginning of the descriptions of all the algorithms, with links to the research papers that introduced them and discussions of their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-throughput Architectures\n",
    "\n",
    "* [Distributed Prioritized Experience Replay (Ape-X)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x)\n",
    "* [Importance Weighted Actor-Learner Architecture (IMPALA)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala)\n",
    "* [Asynchronous Proximal Policy Optimization (APPO)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo)\n",
    "* [Decentralized Distributed Proximal Policy Optimization (DD-PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based\n",
    "\n",
    "* [Advantage Actor-Critic (A2C, A3C)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-actor-critic-a2c-a3c)\n",
    "* [Deep Deterministic Policy Gradients (DDPG, TD3)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg-td3)\n",
    "* [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn)\n",
    "* [Policy Gradients](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#policy-gradients)\n",
    "* [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo)\n",
    "* [Soft Actor-Critic (SAC)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#soft-actor-critic-sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-free\n",
    "\n",
    "* [Augmented Random Search (ARS)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#augmented-random-search-ars)\n",
    "* [Evolution Strategies](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#evolution-strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-agent Specific\n",
    "\n",
    "* [QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn)\n",
    "* [Multi-Agent Deep Deterministic Policy Gradient (contrib/MADDPG)](https://docs.ray.io/en/latest/rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-contrib-maddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline\n",
    "\n",
    "* [Advantage Re-Weighted Imitation Learning (MARWIL)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Bandits (contrib/bandits)\n",
    "\n",
    "* [Linear Upper Confidence Bound (contrib/LinUCB)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-upper-confidence-bound-contrib-linucb)\n",
    "* [Linear Thompson Sampling (contrib/LinTS)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-thompson-sampling-contrib-lints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "* [Single-Player Alpha Zero (contrib/AlphaZero)](https://docs.ray.io/en/latest/rllib-algorithms.html#single-player-alpha-zero-contrib-alphazero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Overview](00-Ray-RLlib-Overview.ipynb) for recommendations on which lessons to study next."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
