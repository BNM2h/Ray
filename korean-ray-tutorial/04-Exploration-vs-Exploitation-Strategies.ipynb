{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Exploration-Exploitation Strategies\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What strategy should we follow for selecting actions that balance the exploration-exploitation tradeoff, yielding the maximum average reward over time? This is the core challenge of RL/bandit algorithms.\n",
    "\n",
    "This lesson has two goals, to give you an intuitive sense of what makes a good algorithm and to introduce several popular examples. \n",
    "\n",
    "> **Tip:** For the first time through this material, you may wish to focus on the first goal, developing an intuitive sense of the requirements for a good algorithm. Come back later to explore the details of the algorithms discussed.\n",
    "\n",
    "So, at least read through the first sections, stopping at _UCB in More Detail_ under _Upper Confidence Bound_. \n",
    "\n",
    "\n",
    "---------------------\n",
    "\n",
    "**actions**으로 부터 최대의 보상을 얻기위해 **exploration-exploitation**을 적절하게 이용하기 위해서는 어떤 전략이 필요할까요? 이것이 바로 **RL/bandit**알고리즘의 핵심내용입니다.\n",
    "\n",
    "이번장에서는 좋은 알고리즘을 만들기 위해 필요한 직관적 접근 방법 그리고 몇 가지 유명한 관련 예제들을 소개합니다.\n",
    "\n",
    "> **Tip:** 이번 자료를 통해 독자는 아마 처음으로 좋은 알고리즘을 만들기 위한 직관력을 쌓을 수 있을 것 입니다. 알고리즘에 대한 상세한 내용은 추후에 더 알아보도록 하겠습니다. \n",
    "\n",
    "먼저, 첫 번째 파트를 먼저 읽으시고, 더 자세한 내용은 _Upper Confidence Bound_ 아래 _UCB in More Detail_ 에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes a Good Exploration-Exploitation Algorithm?\n",
    "\n",
    "Let's first assume we are considered only stationary bandits. The ideal algorithm achieves these properties:\n",
    "\n",
    "1. It explores all the actions reasonably aggressively.\n",
    "2. When exploring, it picks the action most likely to produce an optimal reward, rather than making random choices.\n",
    "3. It converges quickly to the action that optimizes the mean reward.\n",
    "4. It stops exploration once the optimal action is known and just exploits!\n",
    "\n",
    "For non-stationary and context bandits, the optimal action will likely change over time, so some exploration may always be needed.\n",
    "\n",
    "---------------------\n",
    "\n",
    "먼저 우리가 다루는 것이 고정적(stationary) **bandits**라고 가정해봅시다. 그렇다면, 이상적인 알고리즘의 특징은 아래와 같습니다:\n",
    "\n",
    "1. 탐험(exploration)을 통해 모든 **action**에 대해서 합리적이면서 적극적으로 탐구합니다.  \n",
    "2. 탐험을 할때, 무작위의 선택을 하기보다는 최적 보상을 얻기 위한 **action**을 더 하려고 할 것입니다.\n",
    "3. 평균적으로 보상을 최적화하는 **action**에 대해 빠르게 수렴합니다.\n",
    "4. 최적의 **action**을 학습하게되면 더 이상 탐험하지 않습니다.\n",
    "\n",
    "고정적이지 않은 (non-stationary) **context bandits**의 경우, 최적의 **action**이 시간에 따라 달라질 수 있기 때문에, 간헐적인 탐색은 항상 필요할 수 도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Algorithms\n",
    "\n",
    "With these properties in mind, let's briefly discuss four algorithms. We'll use two of them in examples over several subsequent lessons.\n",
    "\n",
    "---------------------\n",
    "\n",
    "이러한 특징들을 염두해두고, 간략하게 4가지 알고리즘들에 대해서 알아보겠습니다. 이 중 2개의 알고리즘은 이후 파트에서도 예제로 사용될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy\n",
    "\n",
    "One possible strategy is quite simple, called $\\epsilon$-Greedy, where $\\epsilon$ is small number that determines how frequently exploration is done. The best-known action is exploited most of the time (\"greedily\"), governed by probability $1 - \\epsilon$ (i.e., in percentage terms $100*(1 - \\epsilon)$%). With probability $\\epsilon$, an action is picked at  random in the hopes of finding a new action that provides even better rewards.\n",
    "\n",
    "Typical values of $\\epsilon$ are between 0.01 and 0.1. A larger value, like 0.1, explores more aggressively and finds the optimal policy more quickly, but afterwards the aggressive exploration strategy becomes a liability, as it only selects the optimal action ~90% of the time, continuing excessive exploration that is now counterproductive. In contrast, smaller values, like 0.01, are slower to find the optimal policy, but once found continue to select it ~99% of the time, so over time the mean reward is _higher_ for _smaller_ $\\epsilon$ values, as the optimal action is selected more often.\n",
    "\n",
    "How does $\\epsilon$-Greedy stack up against our desired properties?\n",
    "\n",
    "1. The higher the $\\epsilon$ value, the more quickly the action space is explored.\n",
    "2. It randomly picks the next action, so there is no \"intelligence\" involved in optimizing the choice.\n",
    "3. The higher the $\\epsilon$ value, the more quickly the optimal action is found.\n",
    "4. Just as this algorithm makes no attempt to optimize the choice of action during exploration, it makes no attempt to throttle back exploration when the optimal value is found. \n",
    "\n",
    "To address point 4, you could adopt an enhancement that decays the $\\epsilon$ value over time, rather than keeping it fixed.\n",
    "\n",
    "See [Wikipedia - MAB Approximate Solutions](https://en.wikipedia.org/wiki/Multi-armed_bandit) and [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) for more information. \n",
    "\n",
    "---------------------\n",
    "\n",
    "첫번째 알고리즘은 넷중 가장 간단한 **$\\epsilon$-Greedy** 입니다. 여기서, **$\\epsilon$** 은 탐험(exploration)의 빈도를 결정하는 아주 작은 수를 의미합니다. 학습된 **action**은 $1 - \\epsilon$ 의 확률($100*(1 - \\epsilon)$%)로 실행됩니다. 대신 $\\epsilon$의 확률로, 더 좋은 **reward**을 기대하며, 무작위로 **action**을 결정합니다.  \n",
    "\n",
    "일반적으로 $\\epsilon$은 0.01에서 0.1사이의 값을 가집니다. 0.1과 같이 큰 값의 경우, 더 적극적으로 탐험함으로서 최적의 정책(policy)를 더 빨리 찾기도 하지만, 추후에는 불필요한 탐험을 남발하게 되어 최적의 **action**을 90%만 선택하는 오히려 비생산적이 되기도 합니다. 반대로 0.01과 같인 작은 값의 경우, 최적의 정책을 찾아가는데 조금 시간이 걸리기는 하지만, 나중에 잘 학습된 후에는 99%로 최적의 **action**을 선택하게 됩니다. 따라서, 작은 값의 $\\epsilon$에 대해 평균적인 **reward**는 더 높을 수 있습니다.\n",
    "\n",
    "**$\\epsilon$-Greedy** 방법이 어떤 특징을 가질까요? \n",
    "\n",
    "1. $\\epsilon$의 값이 클수록, **action**은 더 많은 탐험을 하게 됩니다.\n",
    "2. 다음 **action**에 대해서 무작위로 선택하기 때문에, 최적의 선택과 관련된 \"지능(intelligence)\"은 없습니다.\n",
    "3. $\\epsilon$의 값이 클수록, 최적은 **action**은 더 빠르게 찾습니다.\n",
    "4. 탐험하는데 있어서, **action**의 선택 과정에 최적의 조건을 반영하는 것은 아닌 것처럼, 최적의 **action**을 찾은 후에도 탐험을 중단하려고 하지는 않습니다.\n",
    "\n",
    "위의 4가지 점들에서 단점들을 좀 더 보완하기 위해, 고정된 값의 $\\epsilon$을 사용하기 보다는, 시간에 따라 점차 줄여가며 사용할 수 있습니다.\n",
    "\n",
    "좀 더 자세한 내용은 링크를 참조하세요.\n",
    "[Wikipedia - MAB Approximate Solutions](https://en.wikipedia.org/wiki/Multi-armed_bandit) [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence Bound\n",
    "\n",
    "A limitation about $\\epsilon$-greedy is that exploration is done indiscriminately. Is it possible to make a more informed choice about which alternative actions are more likely to yield a good result, so we preferentially pick one of them? That's what the Upper Confidence Bound (UCB) algorithm attempts to do. It weights some choices over others. \n",
    "\n",
    "It's worth looking at the formula that governs the choice for the next action at time $t$:\n",
    "\n",
    "$$A_t \\doteq \\frac{argmax}{a}\\bigg[ Q_t(a) + c\\sqrt{ \\dfrac{\\ln(t)}{N_t(a)} }\\bigg]$$\n",
    "\n",
    "It's not essential to fully understand all the details, but here is the gist of it; the best action to take at time $t$, $A_t$, is decided by picking the best known action for returning the highest value (the $Q_t(a)$ term in the brackets [...] computes this), but with a correction that encourages exploration, especially for smaller $t$, but penalizing particular actions $a$ if we've already picked them a lot previously (the second term starting with a constant $c$ that governs the \"strength\" of this correction).\n",
    "\n",
    "UCB is one of the best performing algorithms [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition). How does it stack up against our desired properties?\n",
    "\n",
    "1. Exploration is reasonably quick, governed by the $c$ hyperparameter for the \"correction term\".\n",
    "2. It attempts to pick a good action when exploring, rather than randomly.\n",
    "3. Finding the optimal action occurs efficiently, governed by the constant $c$.\n",
    "4. The $ln(t)$ factor in the correction term grows more slowly over time relative to the counts $N_t(a)$, so exploration occurs less frequently at longer time scales.\n",
    "\n",
    "Because UCB is based on prior measured results, it is an example of a _Frequentist_ approach that is _model free_, meaning we just measure outcomes, we don't build a model to explain the environment.\n",
    "\n",
    "---------------------\n",
    "\n",
    "**$\\epsilon$-greedy** 의 단점은 탐험(exploration)이 너무 무분별하게 진행된다는 점입니다. 탐험하는데 있어서 알고 있는 정보를 활용하여 좀 더 좋은 **action** 들중 선택을 할 수 있도록 하는 다른 방법은 없을까요? 이 부분을 ** the Upper Confidence Bound (UCB) ** 알고리즘이 **action**의 선택에 있어서 가중치를 부여함으로서 해답을 제시하고자 하였습니다.  \n",
    "\n",
    "다음 차례의 **action**을 결정할 때, 아래의 공식을 사용합니다.\n",
    "\n",
    "$$A_t \\doteq \\frac{argmax}{a}\\bigg[ Q_t(a) + c\\sqrt{ \\dfrac{\\ln(t)}{N_t(a)} }\\bigg]$$\n",
    "\n",
    "모든 것들을 완벽히 이해할 필요는 없지만, 핵심은 다음과 같습니다; 시간 $t$에서의 최적의 **action**, $A_t$는 주로 가장 높은 값(링크된 식에서 [...] $Q_t(a)$)을 반환하는 **action**으로 결정되지만, 보정된 탐험을 할때는, 특히나 작은 $t$에 대해서, 이전에 많이 선택된 특정 **action**들 $a$에 대해서 패널티를 부가하게 됩니다.( 상수항 $c$로 시작하는 두번째 항의 경우, 탐험에 적용되는 보정치의 강도를 나타냅니다.)\n",
    "\n",
    "**UCB**는 좋은 결과를 보여주는 알고리즘들 중 하나 입니다 [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition). 어떤 특징들을 가지고 있을가요?\n",
    "\n",
    "1. 탐험(exploration)이 합리적으로 빠르며, 관련하여 보정상수 $c$라는 하이퍼 파라메터를 가집니다..\n",
    "2. 탐험시, 단순히 무작위로 선택을 하기 보다는, 좀 더 낳은 **action**을 선택하고자 합니다.\n",
    "3. 효율적으로 최적의 **action**을 학습하는데 있어서, 보정상수 $c$가 중요한 역활을 합니다.\n",
    "4. 로그항 $ln(t)$은 시간에 따라서 상대적으로 $N_t(a)$에 비해 느리게 상승하는데, 이는 탐험의 빈도가 시간이 지남에 따라 줄어들게 합니다.\n",
    "\n",
    "**UCB** 의 학습은 이전 결과들을 바탕으로 진행되는데, 이는 **_Frequentist_** 접근방식의 한 가지로 **environment**에 대한 해석을 을 벗어나 오로지 행동 결과로서만 학습하는 **_model free_** 모델방식이다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCB in More Detail\n",
    "\n",
    "Let's explain the equation in more detail. If you are just interested in developing an intuition about strategies, this is a good place to stop and go to the next lesson, [Simple Multi-Armed-Bandit](03-Simple-Multi-Armed-Bandit.ipynb).\n",
    "\n",
    "* $A_t$ is the action we want to select at time $t$, the action that is most likely to produce the best reward or most likely to be worth exploring.\n",
    "* For all the actions we can choose from, we pick the action $a$ that maximizes the formula in the brackets [...].\n",
    "* $Q_t(a)$ is any equation we're using to measure the \"value\" received at time $t$ for action $a$. This is the greedy choice, i.e., the equation that tells us which action $a$ we currently know will give us the highest value. If we never wanted to explore, the second term in the brackets wouldn't exist. $Q_t(a)$ alone would always tell us to pick the best action we already know about. (The use of $Q$ comes from an early RL algorithm called _Q learning_ that models the _value_ returned from actions over time.)\n",
    "* The second term in the brackets is the correction that UCB gives us. As time $t$ increases, the natural log of $t$ also increases, but slower and slower for larger $t$. This is good because we hope we will find the optimal action at some earlier time $t$, so exploration at large $t$ is less useful (as long as the bandit is stationary or slowly changing). However, the denominator, $N_t(a)$ is the number of times we've selected $a$ already. The more times we've already tried $a$, the less \"interesting\" it is to try again, so this term penalizes choosing $a$. Finally, $c$ is a constant, a \"knob\" or _hyperparameter_ that determines how much we weight exploration vs. exploitation.\n",
    "\n",
    "\n",
    "When we use UCB in subsequent lessons, we'll use a simple _linear_ equation for $Q_t(a)$, i.e., something of the form $z = ax + by + c$.\n",
    "\n",
    "See [Wikipedia - MAB Approximate solutions for contextual bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit), [these references](../06-RL-References.ipynb#Upper-Confidence-Bound), and the [RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb) for more information. \n",
    "\n",
    "---------------------\n",
    "\n",
    "위의 식에 대해서 좀 더 자세하게 살펴보겠습니다. 이 방식에 대해서 좀 더 직관적으로 이해하고 싶다면, 바로 다음 장 [Simple Multi-Armed-Bandit](03-Simple-Multi-Armed-Bandit.ipynb)으로 넘어가도 괜찮습니다.\n",
    "\n",
    "* $A_t$ 는 시간 $t$에서의 **action**을 의미하며, 가장 좋은 **reward**를 얻고자 혹은 좀 더 의미있는 탐험을 하고자 합니다.\n",
    "* 선택할 수 있는 모든 **action**들 중에서 위의 식[...]을 최대화하는 것을 선택할 것 입니다.\n",
    "* $Q_t(a)$는 시간 $t$에서 **action**$a$를 통해 얻을 수 있는 가치(value)를 구하는 식입니다. $Q_t(a)$는 현재 학습된 상태에서 어떤 **action**$a$가 가장 높은 **value**를 가지는지 알려주는데 이로 부터 우리는 탐욕적(greedy) 선택을 하게 됩니다. 만약 더 이상 탐험을 하고 싶지 않다면, 식의 두 번째항은 없어도 됩니다. $Q_t(a)$ 혼자는 항상 최적의 **action**을 선택하도록 지시합니다 ($Q_t(a)$은 일찍이 강화학습의 **Q-learning**에서 **action** 으로 부터 가치(value)를 계산하는데 사용되어 왔습니다). \n",
    "* 식의 두 번째 항은 **UCB**에서 보정함으로서의 역활을 하고 있습니다. 시간 $t$가 증가함에 따라서, 로그항도 증가는 하지만, $t$의 증가에 비해 상대적으로 작게 증가합니다. 이는 되도록 이른 시간에 최적의 **action**을 찾아내고, 많은 시간이 지난 후에는 탐험을 적게 하는데 도움이 됩니다 (물론 이때 **bandit**은 고정적(stationary)이거나 변화가 매우 느려야 합니다). 분모항인 $N_t(a)$는 $a$를 이전에 선택한 횟수를 나타내는데, 많이 선택하면 할수록, 후에 다른 것들을 시도해 보기위해 관심도(interesting)가 떨어지게 되고, 이는 $a$를 선택시 패널티를 주는 것으로 이어집니다. 마지막으로 상수 $c$는 하이퍼파라메터 혹은 **\"knob\"** 으로 탐험의 비중을 결정하는 역활을 합니다.  \n",
    "\n",
    "이후 UCB에 대한 예제들에서 $Q_t(a)$를 $z = ax + by + c$와 같은 형식의 선형방정식(_linear_ equation)으로 사용하게 될 겁니다.\n",
    "\n",
    "좀 더 자세한 내용을 알고 싶다면 다음을 참조하세요\n",
    "[Wikipedia - MAB Approximate solutions for contextual bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit), [these references](../06-RL-References.ipynb#Upper-Confidence-Bound), [RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling\n",
    "\n",
    "Thompson sampling, developed in the 30s, is similar to UCB in that it picks the action that is believed to have the highest potential of maximum reward. It is a _Bayesian, model-based_ approach, where the model is the posterior distribution and may incorporate prior belief about the environment.\n",
    "\n",
    "The agent samples weights for each action, using their posterior distributions, and chooses the action that produces the highest reward. Calculating the exact posterior is intractable in most cases, so they are usually approximated. Hence, the algorithm models beliefs about the problem. Then, during each iteration, the agent initializes with a random belief acts acts optimally based on it.\n",
    "\n",
    "One trade-off is that Thompson Sampling requires an accurate model of the past policy and may suffer from large variance when the past policy differs significantly from a policy being evaluated. You may observe this if you rerun experiments in subsequent lessons that use Thompson Sampling. The graphs of rewards and especially the ranges from high to low, may change significantly from run to run.\n",
    "\n",
    "Relatively speaking, the Thompson Sampling exploration strategies are newer than UCB and tend to perform better (as we'll see in subsequent lessons), although the math for their theoretical performance is less rigorous than for UCB.\n",
    "\n",
    "For more information, see [Wikipedia](https://en.wikipedia.org/wiki/Thompson_sampling), [A Tutorial on Thompson Sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf), [RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints), and other references in [RL References](../References-Reinforcement-Learning.ipynb).\n",
    "\n",
    "---\n",
    "30년대에 개발된 톰슨 샘플링은 최대 보상을 받을 수 있는 가능성이 가장 높은 행동을 선택한다는 점에서 UCB와 유사합니다. 이것은 모델기반의 접근법, 베이지안 모델입니다. 이 모델은 사후확률 분포이며 환경에 대한 사전확률 분포를 포함할 수 있습니다.\n",
    "\n",
    "agent는 사후 분포를 사용하여 각 action에 대한 가중치를 샘플링하고, 가장 높은 보상을 생성하는 action을 선택합니다. 정확한 사후확률을 계산하는 것은 대개 난해하기 때문에 대부분의 경우 근사치로 계산됩니다. 따라서 알고리즘은 그 문제에 대한 신뢰도를 모델링합니다. 그 다음 각 iteration 동안, agent는 랜덤한 belief action이 이를 기반으로 최적으로 수행되도록 초기화합니다.\n",
    "\n",
    "한 가지 트레이드 오프는 톰슨 샘플링이 과거 정책의 정확한 모델을 필요로하며, 과거 정책이 평가되는 정책과 크게 다를 때 큰 분산을 겪을 수 있다는 것입니다. 톰슨 샘플링을 사용하는 다음 수업에서 실험을 다시 실행하면 이를 관찰할 수 있습니다. 보상의 그래프, 특히 최대에서 최소 범위는 실행할 때마다 많이 다르게 나올 수 있다.\n",
    "\n",
    "상대적으로 톰슨 샘플링 탐험 전략은 UCB보다 더 최신이고 (다음 수업에서 보겠지만) 더 좋은 결과를 보여주는 경향이 있습니다. 비록 톰슨 샘플링의 이론에 대한 수학이 UCB보다 덜 엄격하지만 말입니다.\n",
    "\n",
    "좀 더 자세한 내용을 알고 싶다면 다음을 참조하세요\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Thompson_sampling), [A Tutorial on Thompson Sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf), [RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints), [RL References](../References-Reinforcement-Learning.ipynb).\n",
    "\n",
    "추가적인 한국어 포스팅으로는 송호연님의 [톰슨 샘플링 for Bandits](https://brunch.co.kr/@chris-song/66)이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Bandit Algorithms\n",
    "\n",
    "Focusing explicitly on rewards isn't the only approach. What if we use a more general measure, a _preference_, for selecting an action $a$ at time $t$? We'll use $H_t(a)$ to represent this preference at time $t$ for action $a$. We need to model this so we have a probability of selecting an action $a$. Using the _soft-max distribution_ works, also known as the Gibbs or Boltzmann distribution:\n",
    "\n",
    "$Pr\\{A_t = a\\} \\doteq \\frac{e^{H_t(a)}}{\\sum^{k}_{b=1}e^{H_t(b)}} \\doteq \\pi_t(a)$\n",
    "\n",
    "$\\pi_t(a)$ is defined to encapsulate this formula for the probability of taking action $a$ at time $t$.\n",
    "\n",
    "The term _gradient_ is used for this algorithm because the training update formula for $H_t(a)$ is very similar to the _stochastic gradient descent_ formula used in other ML problems. \n",
    "\n",
    "---\n",
    "보상에 명시적으로 집중하는 것만이 유일한 접근법은 아닙니다. 만약 우리가 한 번에 행동을 선택하기 위해 조금 더 일반적인 척도인 선호도(_preference_)를 사용한다면 어떨까요? 우리는 시간 $t$에서 행동 $a$에 대한 선호도를 나타내기 위해 $H_t(a)$를 사용할 것입니다. 우리는 이것을 모델화해서 행동 a를 선택할 확률을 구해야 합니다. 깁스 또는 볼츠만 분포라고도하는 소프트 맥스를 이용합니다.\n",
    "\n",
    "After an action $A_t$ is selected at a time $t$ and reward $R_t$ is received, the action preferences are updated as follows:\n",
    "\n",
    "$ H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha(R_t - \\overset{\\_}{R_t})(1 - \\pi_t(A_t))$,  and\n",
    "\n",
    "$ H_{t+1}(a) \\doteq H_t(a) - \\alpha(R_t - \\overset{\\_}{R_t})(\\pi_t(a))$,  for all $a \\ne A_t$\n",
    "\n",
    "where $H_0(a)$ values are initialized to zero, $\\alpha > 0$ is a step size parameter and $\\overset{\\_}{R_t}$ is the average of all the rewards up through and including time $t$. Note that if $R_t - \\overset{\\_}{R_t}$ is positive, meaning the current reward is larger than the average, the preference $H(A_t)$ increases. Otherwise, it decreases. \n",
    "\n",
    "Note the plus vs. minus signs in the two equations before the $\\alpha$ term. If our preference for $A_t$ increases, our preferences for the other actions should decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "행동 $A_t$가 시간 $t$에서 선택되고 보상 $R_t$를 받으면, 행동 선호도는 다음과 같이 업데이트됩니다.:\n",
    "\n",
    "$ H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha(R_t - \\overset{\\_}{R_t})(1 - \\pi_t(A_t))$\n",
    "\n",
    "$ H_{t+1}(a) \\doteq H_t(a) - \\alpha(R_t - \\overset{\\_}{R_t})(\\pi_t(a))$,  for all $a \\ne A_t$\n",
    "\n",
    "\n",
    "여기서 $H_0(a)$는 0으로 초기화되고, $\\alpha$는 양수인 스텝 사이즈를 나타내는 파라미터 입니다. $\\overset{\\_}{R_t}$는 시간 $t$를 포함한 모든 보상의 평균입니다. 만약 $R_t - \\overset{\\_}{R_t}$가 양수이면 현재 보상이 평균보다 크다는 것을 의미하며, 선호도인 $H(A_t)$는 증가합니다. 그렇지 않다면 감소합니다.\n",
    "\n",
    "$\\alpha$ 항 이전의 두 방정식의 `+` 대 `-` 기호에 주목하세요. $A_t$에 대한 선호도가 높아지면 다른 행동에 대한 선호도가 낮아져야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Thompson Sampling satisfy our desired properties?\n",
    "\n",
    "1. As shown, this algorithm doesn't have tuning parameters to control the rate of exploration or convergence to the optimal solution. However, the convergence is reasonably quick if the variance in reward values is relatively high, so that the difference $R_t - \\overset{\\_}{R_t}$ is also relatively large for low $t$ values.\n",
    "2. It attempts to pick a good action when exploring, rather than randomly.\n",
    "3. See 1.\n",
    "4. As $\\overset{\\_}{R_t}$ converges to a maximum, the difference $R_t - \\overset{\\_}{R_t}$ and hence all the preference values $H_t(a)$ will become relatively stationary, with the optimal action having the highest $H$. Since the $H_t(a)$ values govern the probability of being selected, based on the _soft-max distribution_, if the optimal action has a significantly higher $H_t(a)$ than the other actions, it will be chosen most frequently. If the differences between $H_t(a)$ values are not large, then several will be chosen frequently, but that also means their rewards are relatively close. Hence, in either case, the average reward over time will still be close to optimal.\n",
    "\n",
    "There are many more details about Thompson Sampling, but we won't discuss them further here. See [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) for the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "톰슨 샘플링은 우리가 원하는 속성을 어떻게 만족시킵니까?\n",
    "\n",
    "1. 이 알고리즘은 최적의 솔루션에 대한 탐색 또는 수렴 속도를 제어하기 위한 튜닝 매개 변수가 없습니다. 그러나 보상값의 분산이 상대적으로 높으면 수렴이 상당히 빨라서 $R_t - \\overset{\\_}{R_t}$이 작은 t값에 대해서는 비교적 급니다.\n",
    "2. 무작위로 탐사하기보다는 탐사할 때 좋은 행동을 선택하려는 시도를 합니다.\n",
    "3. 1을 참조하세요.\n",
    "4. $\\overset{\\_}{R_t}$가 최대값으로 수렴함에 따라, $R_t - \\overset{\\_}{R_t}$와 모든 선호도 값 $H_t(a)$는 상대적으로 정적이게 되고 최적의 행동은 가장 높은 H를 가지게 됩니다. $H_t(a)$는 soft-max 분포를 기반으로 선택될 확률을 지배하기 때문에, 최적의 행동이 다른 행동보다 $H_t(a)$가 현저히 높은 경우, 가장 자주 선택될 것입니다. $H_t(a)$ 값의 차이가 크지 않으면 여러 행동들이 자주 선택되겠지만, 그러나 이는 또한 그들의 보상이 상대적으로 가깝다는 것을 의미합니다. 따라서 어느 경우든 시간이 지남에 따라 평균 보상은 여전히 최적에 가까워질 것입니다.\n",
    "\n",
    "톰슨 샘플링에 대해 이제 더 이상 논의하지 않을 것입니다. 더 자세한 내용은 [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition)에서 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
