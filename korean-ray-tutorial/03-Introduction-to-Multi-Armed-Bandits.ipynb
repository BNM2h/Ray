{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Introduction to Bandits\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Multi-Armed Bandits?\n",
    "\n",
    "A **multi-armed bandit (MAB)** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit)) is one way to implement **online machine learning**, based on probability theory. Online machine learning is the idea of updating models as data arrives, rather than training \"offline\" using batch processes and historical data. Hence, a MAB provides a means to inform sequential decision-making about a process over time in the face of uncertainty. An analogous real-world situation is the way an investment fund manager adjusts a stock portfolio in response to evolving market conditions. \n",
    "\n",
    "Multi-armed bandits are named after slot machines in casinos (a.k.a., _one-armed bandits_) where pulling an arm probabilistically produces a cash payout as a reward (or fails to do so). In a typical MAB implementation, a bandit will have N “arms”, one for each of N possible actions to take at a given time step.\n",
    "\n",
    "For simplicity, we'll often just say \"bandits\" to mean MABs.\n",
    "\n",
    "---------------------\n",
    "\n",
    "**MAB**는 카지노에서 슬롯머신의 팔을 당겨서 최고의 이익을 내는 도박에(?) 기반하여 만들어졌다. \n",
    "* 확률론에 기반을 두고 있으며, Online learning 알고리즘이다\n",
    "* 모르는 환경에서의 순차적 의사 결정 문제\n",
    "* bandits가 슬롯머신 팔인거 같다...\n",
    "\n",
    "[참고 블로그](https://brunch.co.kr/@chris-song/62)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandits vs. General Reinforcement Learning\n",
    "\n",
    "Bandits, in their simplest form, are have two restrictions compared to general reinforcement learning:\n",
    "\n",
    "1. They do not attempt to learn the best actions to take for more than one state, which is either stationary or slowly evolving. \n",
    "2. The action taken only affects the reward received, but not the next state of the environment. Recall from our [01 Introduction to Reinforcement Learning](../01-Introduction-to-Reinforcement-Learning.ipynb) lesson that RL is based on _Markov Decision Processes_, where the next state of the system is determined in part by the action taken.\n",
    "\n",
    "\n",
    "------------------------\n",
    "\n",
    "**강화학습과 MAD의 차이점**\n",
    "\n",
    "* 정적으로 학습한다. 여러가지 상태에서의 최적의 행동을 배우지 않는다\n",
    "* MAD에서 action은 다음 상태에 영향을 주지 않는다. 하지만 리워드는 받는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits\n",
    "\n",
    "A **contextual bandit** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit), [Ray documentation](https://docs.ray.io/en/latest/rllib-algorithms.html#contextual-bandits-contrib-bandits)) relaxes the first restriction by adding access to a state of the environment, which is called the _context_, that influences the decisions that are made. At each time step, the algorithm observes this context, which can change significantly from one step to the next, and decides to choose one action among the available alternatives, then observes the outcome/reward of that decision. The environment can change randomly between steps.\n",
    "\n",
    "Returning to the inspiration of slot machines, a machine might change the images or colors it shows on the display to indicate a new context. Internally, this might involve a change in the probability of a payout. This change could happen between each pull of the arm.\n",
    "\n",
    "So, contextual bandits are a step closer to full RL, but the second difference remains, that the state is not influenced by the action taken.\n",
    "\n",
    "[Sutton 2018](../06-RL-References.ipynb#Books) uses the term _situation_ as a generic term for the environment's state and contextual bandits are called _associative search_ bandits, because they search for the best actions, but associated to a particular state (situation) value. Hence, non-contextual bandits are called _non-associative_. \n",
    "\n",
    "-----------------------------\n",
    "\n",
    "**contextual bandit**\n",
    "\n",
    "* 매 이터레이션 마다 arm들 중에 하나를 선택한다. feature vector (context vector)를 보고 선택한다. 과거의 모든 context vector와 보상 /(trajectory)/을 기반으로 현재 arm을 선택한다. \n",
    "* 환경은 이터레이션 도중 랜덤하게 바뀔 수 있다. -> feature vector가 바뀐다는 말, 슬롯머신에서 색이나 화면이 바뀌는.. 그런 것\n",
    "* 강화학습에 가까움. 하지만 여전히 Action에 의해서 상태가 바뀌지않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward vs. Regret\n",
    "\n",
    "While it's common in reinforcement learning to maximize **rewards**, we can also model the opposite of reward, called **regret**. Essentially, regret is the difference between the reward actually achieved and the best possible reward, if the algorithm could choose the best decision at every step.\n",
    "\n",
    "The point of using a bandit is to try to _maximize average reward_. If the algorithm had perfect visibility into the dynamics of the environment, it would always choose the optimal arm to maximize the cumulative reward! However, that's not an interesting real-world problem to solve. Instead, we focus on the case where the algorithm only has limited visibility, so it cannot predict rewards. This means that the **exploitation vs. exploration tradeoff** is an essential aspect of bandits that is shared with general reinforcement learning systems. Pulling one arm repeatedly may provide an excellent reward, but other arms my provide even better rewards, especially as the context evolves.\n",
    "\n",
    "Even while the reward can't be predicted, some the exploration-exploitation strategies can place theoretical upper bounds on the regret. The use of machine learning to leverage the context and then minimize the regret in turn can provide good solutions. We'll explore this idea in the lessons.\n",
    "\n",
    "---------------------\n",
    "\n",
    "강화학습은 보상을 최대화한다. 보상의 반대로 regret이 사용된다.\n",
    "최대의 보상을 받지 못하면, 그 차이가 regret이고, 매 스텝마다 받게된다. \n",
    "\n",
    "bandit에서의 중점은 평균 보상을 최대화 하는 방향으로 문제를 푸는 것이지만, 완벽한 visibility가 있을 때 만 가장 최적의 알고리즘으로 갈 것 이다. \n",
    "이는 별로 흥미로운 문제가 아니라서..~~바로 최적의 답을 알 수 있을테니??~~  완벽하지 않은 visibility 문제를 풀 때 보상을 추측할 수 있으니 ~~이런 알고리즘을 쓰겠지~~.\n",
    "그래서 이것은 강화학습과 마찬가지로 **exploitation vs. exploration tradeoff** 이 중요해진다. \n",
    "\n",
    "보상을 추측 할 수 없어도, 어떤 exploration-exploitation strategies는 이론적인 regret의 상한을 제한한다.인공지능을 regret을 최소화 할 수 있게 사용하여 답을 내면 좋은 답이 될 것이다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating these concepts to the investment fund example, market conditions define the context. Imagine having one arm of the contextual bandit for each of the stocks that could be selected as an investment. For example, when a particular arm gets “pulled” $10,000 gets invested into that stock for that week. Subsequent performance of the selected stocks defines the _reward_. The difference between this reward and the subsequent performance of the best-performing stock in the market defines the _regret_.\n",
    "\n",
    "Of course, this example grossly oversimplified how investment fund management actually works, but it provides a good idea of how contextual bandits can be used for online learning problems. Indeed, sophisticated bandit models are used in finance.\n",
    "\n",
    "---------------\n",
    "\n",
    "투자를 예시로 들면, 시장이 context가 될 것 이다.\n",
    "\n",
    "arm들이 각각의 주식이 되고, 그중 하나의 주식이 그주에 $10,000의 투자를 받게 된다고 해보자. \n",
    "그 주식을 산 선택의 reward는 후속 이익이 될 것 이고, 이론적인 최대 이익과의 차가 regret이 될 것이다. \n",
    "\n",
    "복잡한 bandit model은 경제학에 사용된다고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology Summary\n",
    "\n",
    "Let's summarize terms we've defined already and review RL terms we already know:\n",
    "\n",
    "* **State:** The state of the environment is its “context”.\n",
    "* **Action:** The decision taken, i.e., which arm is pulled.\n",
    "* **Agent:** Same as before; the agent decides what action to take, based on a policy for the environment.\n",
    "* **Reward:** The observed reward received by taking the last action and the cumulative rewards for all actions.\n",
    "* **Regret:** The difference between the reward that could be achieved by always selecting the optimal arm and the reward actually achieved by the algorithm.\n",
    "* **Exploration/exploitation tradeoff:** Balancing when to exploit the best rewarding arm known vs. trying different choices to learn more about the possibility of even better rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Bandits have been applied to a wide variety of applications. Another practical use of bandits is testing alternative designs (like websites), often with greater flexibility and less cost than conventional A/B testing. Many example uses are described in [this paper](https://arxiv.org/abs/1904.10040):\n",
    "\n",
    "1. Determining the correct doses for individuals of widely-used medications, where the \"context\" is the individual's health data.\n",
    "2. Modeling brain and behaviorial disorders, accounting for sequential patterns.\n",
    "3. Portfolio management, balancing risk vs. reward and accounting for correlated behaviors between \"instruments\" (e.g., particular stocks).\n",
    "4. Dynamic pricing of goods and services based on demand and feature sets.\n",
    "5. Recommendation systems when the user and item data sets are too large for traditional collaborative filtering approaches and also to better account for users' evolving preferences.\n",
    "6. Information retrieval that accounts for iterative retrieval and likely exploration paths using the results.\n",
    "7. Dialog systems, where the sequential, online aspect of MABs is a better fit for dialogs than offline supervised learning, and also enables the dialog systems to continue learning as they interact with users.\n",
    "8. Anomaly detection, where the exploitation, exploration tradeoff informs how suspected anomalies are investigated, including which ones to pursue or ignore, based on relative likely significance.\n",
    "9. Frequency hopping, \"spreading\", and power control in telecommunications applications modeled and optimized to improve the end user experience."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
